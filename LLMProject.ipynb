{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZeUonic26WPe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch import optim\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import io\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "#!pip install SumOfSquares\n",
        "\n",
        "from parser import get_parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_MAX_POSITIONS = 4096  # maximum input sequence length\n",
        "\n",
        "\n",
        "def Embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
        "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
        "    nn.init.normal_(m.weight, mean=0, std=embedding_dim**-0.5)\n",
        "    if padding_idx is not None:\n",
        "        nn.init.constant_(m.weight[padding_idx], 0)\n",
        "    return m\n",
        "\n",
        "\n",
        "def create_sinusoidal_embeddings(n_pos, dim, out):\n",
        "    position_enc = np.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])\n",
        "    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "    out.detach_()\n",
        "    out.requires_grad = False\n",
        "\n",
        "\n",
        "def get_masks(slen, lengths, causal):\n",
        "    \"\"\"\n",
        "    Generate hidden states mask, and optionally an attention mask.\n",
        "    \"\"\"\n",
        "    assert lengths.max().item() <= slen\n",
        "    bs = lengths.size(0)\n",
        "    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n",
        "    mask = alen < lengths[:, None]\n",
        "\n",
        "    # attention mask is the same as mask, or triangular inferior attention (causal)\n",
        "    if causal:\n",
        "        attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]\n",
        "    else:\n",
        "        attn_mask = mask\n",
        "\n",
        "    # sanity check\n",
        "    assert mask.size() == (bs, slen)\n",
        "    assert causal is False or attn_mask.size() == (bs, slen, slen)\n",
        "\n",
        "    return mask, attn_mask\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    NEW_ID = itertools.count()\n",
        "\n",
        "    def __init__(self, n_heads, dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layer_id = next(MultiHeadAttention.NEW_ID)\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "        assert self.dim % self.n_heads == 0\n",
        "\n",
        "        self.q_lin = nn.Linear(dim, dim)\n",
        "        self.k_lin = nn.Linear(dim, dim)\n",
        "        self.v_lin = nn.Linear(dim, dim)\n",
        "        self.out_lin = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, input, mask, kv=None, use_cache=False):\n",
        "        \"\"\"\n",
        "        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n",
        "        Input is (bs, qlen, dim)\n",
        "        Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n",
        "        \"\"\"\n",
        "        assert not (use_cache and self.cache is None)\n",
        "        bs, qlen, dim = input.size()\n",
        "        if kv is None:\n",
        "            klen = qlen if not use_cache else self.cache[\"slen\"] + qlen\n",
        "        else:\n",
        "            klen = kv.size(1)\n",
        "        assert dim == self.dim, \"Dimensions do not match: %s input vs %s configured\" % (dim, self.dim)\n",
        "        n_heads = self.n_heads\n",
        "        dim_per_head = dim // n_heads\n",
        "        mask_reshape = (bs, 1, qlen, klen) if mask.dim() == 3 else (bs, 1, 1, klen)\n",
        "\n",
        "        def shape(x):\n",
        "            \"\"\"projection\"\"\"\n",
        "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
        "\n",
        "        def unshape(x):\n",
        "            \"\"\"compute context\"\"\"\n",
        "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
        "\n",
        "        q = shape(self.q_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n",
        "        if kv is None:\n",
        "            k = shape(self.k_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n",
        "            v = shape(self.v_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n",
        "        elif not use_cache or self.layer_id not in self.cache:\n",
        "            k = v = kv\n",
        "            k = shape(self.k_lin(k))  # (bs, n_heads, qlen, dim_per_head)\n",
        "            v = shape(self.v_lin(v))  # (bs, n_heads, qlen, dim_per_head)\n",
        "\n",
        "        if use_cache:\n",
        "            if self.layer_id in self.cache:\n",
        "                if kv is None:\n",
        "                    k_, v_ = self.cache[self.layer_id]\n",
        "                    k = torch.cat([k_, k], dim=2)  # (bs, n_heads, klen, dim_per_head)\n",
        "                    v = torch.cat([v_, v], dim=2)  # (bs, n_heads, klen, dim_per_head)\n",
        "                else:\n",
        "                    k, v = self.cache[self.layer_id]\n",
        "            self.cache[self.layer_id] = (k, v)\n",
        "\n",
        "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, dim_per_head)\n",
        "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, qlen, klen)\n",
        "        mask = (mask == 0).view(mask_reshape).expand_as(scores)  # (bs, n_heads, qlen, klen)\n",
        "        scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, qlen, klen)\n",
        "\n",
        "        weights = F.softmax(scores.float(), dim=-1).type_as(scores)  # (bs, n_heads, qlen, klen)\n",
        "        weights = F.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)\n",
        "        context = torch.matmul(weights, v)  # (bs, n_heads, qlen, dim_per_head)\n",
        "        context = unshape(context)  # (bs, qlen, dim)\n",
        "\n",
        "        if TransformerModel.STORE_OUTPUTS and not self.training:\n",
        "            self.outputs = weights.detach().cpu()\n",
        "\n",
        "        return self.out_lin(context)\n",
        "\n",
        "\n",
        "class TransformerFFN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, dim_hidden, out_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.lin1 = nn.Linear(in_dim, dim_hidden)\n",
        "        self.lin2 = nn.Linear(dim_hidden, out_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.lin1(input)\n",
        "        x = F.relu(x)\n",
        "        x = self.lin2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    STORE_OUTPUTS = False\n",
        "\n",
        "    def __init__(self, params, id2word, is_encoder, with_output):\n",
        "        \"\"\"\n",
        "        Transformer model (encoder or decoder).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder / decoder, output layer\n",
        "        self.dtype = torch.half if params.fp16 else torch.float\n",
        "        self.is_encoder = is_encoder\n",
        "        self.is_decoder = not is_encoder\n",
        "        self.with_output = with_output\n",
        "        self.params = params\n",
        "\n",
        "        # dictionary\n",
        "        self.n_words = len(id2word)\n",
        "        self.eos_index = 0\n",
        "        self.pad_index = 1\n",
        "        self.id2word = id2word\n",
        "\n",
        "        # model parameters\n",
        "        self.dim = params.emb_dim  # 512 by default\n",
        "        self.hidden_dim = self.dim * 4  # 2048 by default\n",
        "        self.n_heads = params.n_heads  # 8 by default\n",
        "        self.n_layers = params.n_enc_layers if is_encoder else params.n_dec_layers\n",
        "        self.dropout = params.dropout\n",
        "        self.attention_dropout = params.attention_dropout\n",
        "        self.max_src_len = params.max_src_len\n",
        "        assert self.dim % self.n_heads == 0, \"transformer dim must be a multiple of n_heads\"\n",
        "\n",
        "        # embeddings\n",
        "        self.position_embeddings = Embedding(N_MAX_POSITIONS, self.dim)\n",
        "        if params.sinusoidal_embeddings:\n",
        "            create_sinusoidal_embeddings(N_MAX_POSITIONS, self.dim, out=self.position_embeddings.weight)\n",
        "        self.embeddings = Embedding(self.n_words, self.dim, padding_idx=self.pad_index)\n",
        "        self.layer_norm_emb = nn.LayerNorm(self.dim, eps=1e-12)\n",
        "\n",
        "        # transformer layers\n",
        "        self.attentions = nn.ModuleList()\n",
        "        self.layer_norm1 = nn.ModuleList()\n",
        "        self.ffns = nn.ModuleList()\n",
        "        self.layer_norm2 = nn.ModuleList()\n",
        "        if self.is_decoder:\n",
        "            self.layer_norm15 = nn.ModuleList()\n",
        "            self.encoder_attn = nn.ModuleList()\n",
        "\n",
        "        for layer_id in range(self.n_layers):\n",
        "            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))\n",
        "            self.layer_norm1.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "            if self.is_decoder:\n",
        "                self.layer_norm15.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "                self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))\n",
        "            self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, dropout=self.dropout))\n",
        "            self.layer_norm2.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "        # output layer\n",
        "        if self.with_output:\n",
        "            self.proj = nn.Linear(self.dim, self.n_words, bias=True)\n",
        "            if params.share_inout_emb:\n",
        "                self.proj.weight = self.embeddings.weight\n",
        "\n",
        "    def forward(self, mode, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward function with different forward modes.\n",
        "        ### Small hack to handle PyTorch distributed.\n",
        "        \"\"\"\n",
        "        if mode == \"fwd\":\n",
        "            return self.fwd(**kwargs)\n",
        "        elif mode == \"predict\":\n",
        "            return self.predict(**kwargs)\n",
        "        else:\n",
        "            raise Exception(\"Unknown mode: %s\" % mode)\n",
        "\n",
        "    def fwd(self, x, lengths, causal, src_enc=None, src_len=None, positions=None, use_cache=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            `x` LongTensor(slen, bs), containing word indices\n",
        "            `lengths` LongTensor(bs), containing the length of each sentence\n",
        "            `causal` Boolean, if True, the attention is only done over previous hidden states\n",
        "            `positions` LongTensor(slen, bs), containing word positions\n",
        "        \"\"\"\n",
        "        # lengths = (x != self.pad_index).float().sum(dim=1)\n",
        "        # mask = x != self.pad_index\n",
        "\n",
        "        # check inputs\n",
        "        slen, bs = x.size()\n",
        "        assert lengths.size(0) == bs\n",
        "        assert lengths.max().item() <= slen\n",
        "        x = x.transpose(0, 1)  # batch size as dimension 0\n",
        "        assert (src_enc is None) == (src_len is None)\n",
        "        if src_enc is not None:\n",
        "            assert self.is_decoder\n",
        "            assert src_enc.size(0) == bs\n",
        "        assert not (use_cache and self.cache is None)\n",
        "\n",
        "        # generate masks\n",
        "        mask, attn_mask = get_masks(slen, lengths, causal)\n",
        "        if self.is_decoder and src_enc is not None:\n",
        "            if self.max_src_len > 0:\n",
        "                src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < torch.clamp(src_len[:, None], max=self.max_src_len)\n",
        "            else:\n",
        "                src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]\n",
        "\n",
        "        # positions\n",
        "        if positions is None:\n",
        "            positions = x.new(slen).long()\n",
        "            positions = torch.arange(slen, out=positions).unsqueeze(0)\n",
        "        else:\n",
        "            assert positions.size() == (slen, bs)\n",
        "            positions = positions.transpose(0, 1)\n",
        "\n",
        "        # do not recompute cached elements\n",
        "        if use_cache:\n",
        "            _slen = slen - self.cache[\"slen\"]\n",
        "            x = x[:, -_slen:]\n",
        "            positions = positions[:, -_slen:]\n",
        "            mask = mask[:, -_slen:]\n",
        "            attn_mask = attn_mask[:, -_slen:]\n",
        "\n",
        "        # all layer outputs\n",
        "        if TransformerModel.STORE_OUTPUTS and not self.training:\n",
        "            self.outputs = []\n",
        "\n",
        "        # embeddings\n",
        "        tensor = self.embeddings(x)\n",
        "        tensor = tensor + self.position_embeddings(positions).expand_as(tensor)\n",
        "        tensor = self.layer_norm_emb(tensor)\n",
        "        tensor = F.dropout(tensor, p=self.dropout, training=self.training)\n",
        "        tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "        if TransformerModel.STORE_OUTPUTS and not self.training:\n",
        "            self.outputs.append(tensor.detach().cpu())\n",
        "\n",
        "        # transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            # self attention\n",
        "            self.attentions[i].cache = self.cache\n",
        "            attn = self.attentions[i](tensor, attn_mask, use_cache=use_cache)\n",
        "            attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "            tensor = tensor + attn\n",
        "            tensor = self.layer_norm1[i](tensor)\n",
        "\n",
        "            # encoder attention (for decoder only)\n",
        "            if self.is_decoder and src_enc is not None:\n",
        "                self.encoder_attn[i].cache = self.cache\n",
        "                attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, use_cache=use_cache)\n",
        "                attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "                tensor = tensor + attn\n",
        "                tensor = self.layer_norm15[i](tensor)\n",
        "\n",
        "            # FFN\n",
        "            tensor = tensor + self.ffns[i](tensor)\n",
        "            tensor = self.layer_norm2[i](tensor)\n",
        "\n",
        "            tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "            if TransformerModel.STORE_OUTPUTS and not self.training:\n",
        "                self.outputs.append(tensor.detach().cpu())\n",
        "\n",
        "        # update cache length\n",
        "        if use_cache:\n",
        "            self.cache[\"slen\"] += tensor.size(1)\n",
        "\n",
        "        # move back sequence length to dimension 0\n",
        "        tensor = tensor.transpose(0, 1)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def predict(self, tensor, pred_mask, y, get_scores, neg_samples=None):\n",
        "        \"\"\"\n",
        "        Given the last hidden state, compute word scores and/or the loss.\n",
        "            `pred_mask` is a ByteTensor of shape (slen, bs), filled with 1 when\n",
        "                we need to predict a word\n",
        "            `y` is a LongTensor of shape (pred_mask.sum(),)\n",
        "            `get_scores` is a boolean specifying whether we need to return scores\n",
        "            `neg_samples` is an optional LongTensor of shape (pred_mask.sum(), n_negatives)\n",
        "                containing negative samples for unlikelihood or contrastive training\n",
        "        \"\"\"\n",
        "        x = tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.dim)\n",
        "        assert (y == self.pad_index).sum().item() == 0\n",
        "        scores = self.proj(x).view(-1, self.n_words)\n",
        "\n",
        "        # Standard cross-entropy loss\n",
        "        ce_loss = F.cross_entropy(scores.float(), y, reduction=\"mean\")\n",
        "\n",
        "        # unlikelihood training parameters\n",
        "        ul_alpha = params.ul_alpha  # weight for unlikelihood loss\n",
        "        ul_topp = params.ul_topp    # top-p sampling for negative candidates\n",
        "        ul_topk = params.ul_topk    # top-k sampling for negative candidates\n",
        "        ul_temp = params.ul_temp    # temperature for negative sampling\n",
        "\n",
        "        # contrastive training parameters\n",
        "        contrastive_alpha = params.contrastive_alpha  # weight for contrastive loss\n",
        "        contrastive_temp = params.contrastive_temp    # temperature for contrastive loss\n",
        "\n",
        "        def generate_neg_samples_ul(scores, topp, topk, temp):\n",
        "          # Generate negative samples automatically\n",
        "          with torch.no_grad():\n",
        "              probs = F.softmax(scores.float() / temp, dim=-1)\n",
        "\n",
        "              if topp > 0:\n",
        "                  # Top-p sampling\n",
        "                  sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "                  cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "                  sorted_indices_to_remove = cumulative_probs > topp\n",
        "                  sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                  sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                  # Zero out low-probability tokens\n",
        "                  probs = probs.scatter(1, sorted_indices, sorted_probs * (~sorted_indices_to_remove).float())\n",
        "                  probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "              # Sample negative candidates\n",
        "              if topk > 0:\n",
        "                  # Top-k sampling\n",
        "                  topk_probs, topk_indices = torch.topk(probs, topk, dim=-1)\n",
        "                  sampled_indices = torch.multinomial(topk_probs, 1).squeeze(-1)\n",
        "                  ul_targets = topk_indices.gather(-1, sampled_indices.unsqueeze(-1)).squeeze(-1)\n",
        "              else:\n",
        "                  # Sample from full distribution\n",
        "                  ul_targets = torch.multinomial(probs, 1).squeeze(-1)\n",
        "\n",
        "          return ul_targets\n",
        "\n",
        "        def _sample_from_scores(scores, topp, topk, temp):\n",
        "          \"\"\"Safe sampling with shape [batch_size, vocab_size]\"\"\"\n",
        "          probs = F.softmax(scores.float() / temp, dim=-1)\n",
        "\n",
        "          if topp > 0:\n",
        "              sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
        "              cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "              mask = cum_probs > topp\n",
        "              mask[..., 1:] = mask[..., :-1].clone()\n",
        "              mask[..., 0] = False\n",
        "              sorted_probs[mask] = 0\n",
        "              sorted_probs.div_(sorted_probs.sum(dim=-1, keepdim=True))\n",
        "              probs = torch.gather(sorted_probs, -1, torch.argsort(sorted_idx, dim=-1))\n",
        "\n",
        "          if topk > 0:\n",
        "              topk_probs, topk_idx = torch.topk(probs, topk, dim=-1)\n",
        "              samples = torch.multinomial(topk_probs, 1)\n",
        "              return topk_idx.gather(-1, samples).squeeze(-1)\n",
        "          else:\n",
        "              return torch.multinomial(probs, 1).squeeze(-1)\n",
        "\n",
        "        def generate_neg_samples_contr(scores, topp=0.9, topk=0, temp=1.0, num_negatives=5):\n",
        "          \"\"\"\n",
        "          Generate negative sequences with strict shape control\n",
        "          Args:\n",
        "              scores: Model logits [batch_size, seq_len, vocab_size]\n",
        "          Returns:\n",
        "              neg_samples: [batch_size, num_negatives, seq_len]\n",
        "          \"\"\"\n",
        "          batch_size = scores.size(0)\n",
        "          seq_len = scores.size(1)\n",
        "          device = scores.device\n",
        "\n",
        "          neg_samples = torch.full((batch_size, num_negatives, seq_len),\n",
        "                                self.pad_index,\n",
        "                                dtype=torch.long,\n",
        "                                device=device)\n",
        "          neg_samples[:, :, 0] = self.eos_index\n",
        "\n",
        "          for n in range(num_negatives):\n",
        "              for i in range(1, seq_len):\n",
        "                  # Current sequence [batch_size, i]\n",
        "                  current_input = neg_samples[:, n, :i]\n",
        "\n",
        "                  with torch.no_grad():\n",
        "                      # Forward pass expects [seq_len, batch_size]\n",
        "                      outputs = self.fwd(\n",
        "                          x=current_input.t(),  # [i, batch_size]\n",
        "                          lengths=torch.full((batch_size,), i, device=device),\n",
        "                          causal=True\n",
        "                      )  # [i, batch_size, dim]\n",
        "\n",
        "                      # Get last token features [batch_size, dim]\n",
        "                      last_hidden = outputs[-1]  # [batch_size, dim]\n",
        "\n",
        "                      # Project to vocab size\n",
        "                      last_scores = self.proj(last_hidden)  # [batch_size, vocab_size]\n",
        "\n",
        "                      # Sample next tokens\n",
        "                      next_tokens = _sample_from_scores(\n",
        "                          last_scores,\n",
        "                          topp=topp,\n",
        "                          topk=topk,\n",
        "                          temp=temp\n",
        "                      )  # [batch_size]\n",
        "\n",
        "                      # Update sequences\n",
        "                      neg_samples[:, n, i] = next_tokens\n",
        "\n",
        "          return neg_samples\n",
        "\n",
        "        # Unlikelihood loss\n",
        "        ul_loss = None\n",
        "        if self.training and (ul_alpha > 0):\n",
        "            if neg_samples is not None:\n",
        "                # Use provided negative samples\n",
        "                ul_targets = neg_samples\n",
        "            else:\n",
        "                ul_targets = generate_neg_samples_ul(scores, ul_topp, ul_topk, ul_temp)\n",
        "\n",
        "            # Compute unlikelihood loss\n",
        "            log_probs = F.log_softmax(scores.float(), dim=-1)\n",
        "            ul_loss = -torch.log(1 - log_probs.exp().gather(1, ul_targets.unsqueeze(-1)) + 1e-5).mean()\n",
        "\n",
        "            # Combine losses\n",
        "            loss = ce_loss + ul_alpha * ul_loss\n",
        "        else:\n",
        "            loss = ce_loss\n",
        "\n",
        "        if contrastive_alpha > 0 :\n",
        "            if neg_samples is None:\n",
        "                neg_samples = generate_neg_samples_contr(scores, ul_topp, ul_topk, ul_temp)\n",
        "                print(neg_samples.shape)\n",
        "            # Get hidden states for negative samples\n",
        "            with torch.no_grad():\n",
        "                neg_tensors = []\n",
        "                for i in range(neg_samples.size(1)):\n",
        "                    neg_seq = neg_samples[:, i]\n",
        "                    neg_tensor = self.fwd(\n",
        "                        x=neg_seq,\n",
        "                        lengths=(neg_seq != self.pad_index).sum(dim=1),\n",
        "                        causal=self.is_decoder,\n",
        "                        positions=None\n",
        "                    )\n",
        "                    neg_tensors.append(neg_tensor)\n",
        "                neg_tensor = torch.stack(neg_tensors, dim=1)  # (seq_len, bs, num_neg, dim)\n",
        "\n",
        "            # Get positive and negative representations\n",
        "            pos_rep = x  # (bs*seq_len, dim)\n",
        "            neg_rep = neg_tensor[pred_mask.unsqueeze(-1).unsqueeze(-1).expand_as(neg_tensor)]\n",
        "            neg_rep = neg_rep.view(-1, neg_samples.size(1), self.dim)  # (bs*seq_len, num_neg, dim)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            pos_sim = F.cosine_similarity(pos_rep, pos_rep, dim=-1)\n",
        "            neg_sim = F.cosine_similarity(pos_rep.unsqueeze(1), neg_rep, dim=-1)\n",
        "\n",
        "            # Temperature-scaled similarities\n",
        "            pos_sim = pos_sim / contrastive_temp\n",
        "            neg_sim = neg_sim / contrastive_temp\n",
        "\n",
        "            # Contrastive loss (infoNCE)\n",
        "            logits = torch.cat([pos_sim, neg_sim], dim=1)\n",
        "            labels = torch.arange(logits.size(0), device=logits.device)\n",
        "            contrastive_loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            # Combine losses\n",
        "            loss = ce_loss + contrastive_alpha * contrastive_loss\n",
        "        else:\n",
        "            loss = ce_loss\n",
        "\n",
        "        return scores, loss\n",
        "\n",
        "    def generate(self, src_enc, src_len, max_len=200, sample_temperature=None):\n",
        "        \"\"\"\n",
        "        Decode a sentence given initial start.\n",
        "        `x`:\n",
        "            - LongTensor(bs, slen)\n",
        "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
        "                <EOS> W1 W2 W3   W4  <EOS>\n",
        "        `lengths`:\n",
        "            - LongTensor(bs) [5, 6]\n",
        "        `positions`:\n",
        "            - False, for regular \"arange\" positions (LM)\n",
        "            - True, to reset positions from the new generation (MT)\n",
        "        \"\"\"\n",
        "\n",
        "        # input batch\n",
        "        bs = len(src_len)\n",
        "        assert src_enc.size(0) == bs\n",
        "\n",
        "        # generated sentences\n",
        "        generated = src_len.new(max_len, bs)  # upcoming output\n",
        "        generated.fill_(self.pad_index)  # fill upcoming ouput with <PAD>\n",
        "        generated[0].fill_(self.eos_index)  # we use <EOS> for <BOS> everywhere\n",
        "\n",
        "        # positions\n",
        "        positions = src_len.new(max_len).long()\n",
        "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand(max_len, bs)\n",
        "\n",
        "        # current position / max lengths / length of generated sentences / unfinished sentences\n",
        "        cur_len = 1\n",
        "        gen_len = src_len.clone().fill_(1)\n",
        "        unfinished_sents = src_len.clone().fill_(1)\n",
        "\n",
        "        # cache compute states\n",
        "        self.cache = {\"slen\": 0}\n",
        "\n",
        "        while cur_len < max_len:\n",
        "\n",
        "            # compute word scores\n",
        "            tensor = self.forward(\n",
        "                \"fwd\",\n",
        "                x=generated[:cur_len],\n",
        "                lengths=gen_len,\n",
        "                positions=positions[:cur_len],\n",
        "                causal=True,\n",
        "                src_enc=src_enc,\n",
        "                src_len=src_len,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            assert tensor.size() == (1, bs, self.dim)\n",
        "            tensor = tensor.data[-1, :, :]  # .to(self.dtype)  # (bs, dim)\n",
        "            scores = self.proj(tensor)  # (bs, n_words)\n",
        "\n",
        "            # select next words: sample or greedy\n",
        "            if sample_temperature is None:\n",
        "                next_words = torch.topk(scores, 1)[1].squeeze(1)\n",
        "            else:\n",
        "                next_words = torch.multinomial(F.softmax(scores.float() / sample_temperature, dim=1), 1).squeeze(1)\n",
        "            assert next_words.size() == (bs,)\n",
        "\n",
        "            # update generations / lengths / finished sentences / current length\n",
        "            generated[cur_len] = next_words * unfinished_sents + self.pad_index * (1 - unfinished_sents)\n",
        "            gen_len.add_(unfinished_sents)\n",
        "            unfinished_sents.mul_(next_words.ne(self.eos_index).long())\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
        "            if unfinished_sents.max() == 0:\n",
        "                break\n",
        "\n",
        "        # add <EOS> to unfinished sentences\n",
        "        if cur_len == max_len:\n",
        "            generated[-1].masked_fill_(unfinished_sents.byte(), self.eos_index)\n",
        "\n",
        "        # sanity check\n",
        "        assert (generated == self.eos_index).sum() == 2 * bs\n",
        "\n",
        "        return generated[:cur_len], gen_len\n",
        "\n",
        "    def generate_beam(self, src_enc, src_len, beam_size, length_penalty, early_stopping, max_len=200):\n",
        "        \"\"\"\n",
        "        Decode a sentence given initial start.\n",
        "        `x`:\n",
        "            - LongTensor(bs, slen)\n",
        "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
        "                <EOS> W1 W2 W3   W4  <EOS>\n",
        "        `lengths`:\n",
        "            - LongTensor(bs) [5, 6]\n",
        "        `positions`:\n",
        "            - False, for regular \"arange\" positions (LM)\n",
        "            - True, to reset positions from the new generation (MT)\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        assert src_enc.size(0) == src_len.size(0)\n",
        "        assert beam_size >= 1\n",
        "\n",
        "        # batch size / number of words\n",
        "        bs = len(src_len)\n",
        "        n_words = self.n_words\n",
        "\n",
        "        # expand to beam size the source latent representations / source lengths\n",
        "        src_enc = src_enc.unsqueeze(1).expand((bs, beam_size) + src_enc.shape[1:]).contiguous().view((bs * beam_size,) + src_enc.shape[1:])\n",
        "        src_len = src_len.unsqueeze(1).expand(bs, beam_size).contiguous().view(-1)\n",
        "\n",
        "        # generated sentences (batch with beam current hypotheses)\n",
        "        generated = src_len.new(max_len, bs * beam_size)  # upcoming output\n",
        "        generated.fill_(self.pad_index)  # fill upcoming ouput with <PAD>\n",
        "        generated[0].fill_(self.eos_index)  # we use <EOS> for <BOS> everywhere\n",
        "\n",
        "        # generated hypotheses\n",
        "        generated_hyps = [BeamHypotheses(beam_size, max_len, length_penalty, early_stopping) for _ in range(bs)]\n",
        "\n",
        "        # positions\n",
        "        positions = src_len.new(max_len).long()\n",
        "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand_as(generated)\n",
        "\n",
        "        # scores for each sentence in the beam\n",
        "        beam_scores = src_enc.new(bs, beam_size).float().fill_(0)\n",
        "        beam_scores[:, 1:] = -1e9\n",
        "        beam_scores = beam_scores.view(-1)\n",
        "\n",
        "        # current position\n",
        "        cur_len = 1\n",
        "\n",
        "        # cache compute states\n",
        "        self.cache = {\"slen\": 0}\n",
        "\n",
        "        # done sentences\n",
        "        done = [False for _ in range(bs)]\n",
        "\n",
        "        while cur_len < max_len:\n",
        "\n",
        "            # compute word scores\n",
        "            tensor = self.forward(\n",
        "                \"fwd\",\n",
        "                x=generated[:cur_len],\n",
        "                lengths=src_len.new(bs * beam_size).fill_(cur_len),\n",
        "                positions=positions[:cur_len],\n",
        "                causal=True,\n",
        "                src_enc=src_enc,\n",
        "                src_len=src_len,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            assert tensor.size() == (1, bs * beam_size, self.dim)\n",
        "            tensor = tensor.data[-1, :, :]  # (bs * beam_size, dim)\n",
        "            scores = self.proj(tensor)  # (bs * beam_size, n_words)\n",
        "            scores = F.log_softmax(scores.float(), dim=-1)  # (bs * beam_size, n_words)\n",
        "            assert scores.size() == (bs * beam_size, n_words)\n",
        "\n",
        "            # select next words with scores\n",
        "            _scores = scores + beam_scores[:, None].expand_as(scores)  # (bs * beam_size, n_words)\n",
        "            _scores = _scores.view(bs, beam_size * n_words)  # (bs, beam_size * n_words)\n",
        "\n",
        "            next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)\n",
        "            assert next_scores.size() == next_words.size() == (bs, 2 * beam_size)\n",
        "\n",
        "            # next batch beam content\n",
        "            # list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)\n",
        "            next_batch_beam = []\n",
        "\n",
        "            # for each sentence\n",
        "            for sent_id in range(bs):\n",
        "\n",
        "                # if we are done with this sentence\n",
        "                done[sent_id] = done[sent_id] or generated_hyps[sent_id].is_done(next_scores[sent_id].max().item())\n",
        "                if done[sent_id]:\n",
        "                    next_batch_beam.extend([(0, self.pad_index, 0)] * beam_size)  # pad the batch\n",
        "                    continue\n",
        "\n",
        "                # next sentence beam content\n",
        "                next_sent_beam = []\n",
        "\n",
        "                # next words for this sentence\n",
        "                for idx, value in zip(next_words[sent_id], next_scores[sent_id]):\n",
        "\n",
        "                    # get beam and word IDs\n",
        "                    beam_id = idx // n_words\n",
        "                    word_id = idx % n_words\n",
        "\n",
        "                    # end of sentence, or next word\n",
        "                    if word_id == self.eos_index or cur_len + 1 == max_len:\n",
        "                        generated_hyps[sent_id].add(generated[:cur_len, sent_id * beam_size + beam_id].clone().cpu(), value.item())\n",
        "                    else:\n",
        "                        next_sent_beam.append((value, word_id, sent_id * beam_size + beam_id))\n",
        "\n",
        "                    # the beam for next step is full\n",
        "                    if len(next_sent_beam) == beam_size:\n",
        "                        break\n",
        "\n",
        "                # update next beam content\n",
        "                assert len(next_sent_beam) == 0 if cur_len + 1 == max_len else beam_size\n",
        "                if len(next_sent_beam) == 0:\n",
        "                    next_sent_beam = [(0, self.pad_index, 0)] * beam_size  # pad the batch\n",
        "                next_batch_beam.extend(next_sent_beam)\n",
        "                assert len(next_batch_beam) == beam_size * (sent_id + 1)\n",
        "\n",
        "            # sanity check / prepare next batch\n",
        "            assert len(next_batch_beam) == bs * beam_size\n",
        "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
        "            beam_words = generated.new([x[1] for x in next_batch_beam])\n",
        "            beam_idx = src_len.new([x[2] for x in next_batch_beam])\n",
        "\n",
        "            # re-order batch and internal states\n",
        "            generated = generated[:, beam_idx]\n",
        "            generated[cur_len] = beam_words\n",
        "            for k in self.cache.keys():\n",
        "                if k != \"slen\":\n",
        "                    self.cache[k] = (self.cache[k][0][beam_idx], self.cache[k][1][beam_idx])\n",
        "\n",
        "            # update current length\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            # stop when we are done with each sentence\n",
        "            if all(done):\n",
        "                break\n",
        "\n",
        "        # select the best hypotheses\n",
        "        tgt_len = src_len.new(bs)\n",
        "        best = []\n",
        "\n",
        "        for i, hypotheses in enumerate(generated_hyps):\n",
        "            best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
        "            tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
        "            best.append(best_hyp)\n",
        "\n",
        "        # generate target batch\n",
        "        decoded = src_len.new(tgt_len.max().item(), bs).fill_(self.pad_index)\n",
        "        for i, hypo in enumerate(best):\n",
        "            decoded[: tgt_len[i] - 1, i] = hypo\n",
        "            decoded[tgt_len[i] - 1, i] = self.eos_index\n",
        "\n",
        "        # sanity check\n",
        "        assert (decoded == self.eos_index).sum() == 2 * bs\n",
        "\n",
        "        return decoded, tgt_len, generated_hyps\n",
        "\n",
        "\n",
        "class BeamHypotheses(object):\n",
        "\n",
        "    def __init__(self, n_hyp, max_len, length_penalty, early_stopping):\n",
        "        \"\"\"\n",
        "        Initialize n-best list of hypotheses.\n",
        "        \"\"\"\n",
        "        self.max_len = max_len - 1  # ignoring <BOS>\n",
        "        self.length_penalty = length_penalty\n",
        "        self.early_stopping = early_stopping\n",
        "        self.n_hyp = n_hyp\n",
        "        self.hyp = []\n",
        "        self.worst_score = 1e9\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of hypotheses in the list.\n",
        "        \"\"\"\n",
        "        return len(self.hyp)\n",
        "\n",
        "    def add(self, hyp, sum_logprobs):\n",
        "        \"\"\"\n",
        "        Add a new hypothesis to the list.\n",
        "        \"\"\"\n",
        "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
        "        if len(self) < self.n_hyp or score > self.worst_score:\n",
        "            self.hyp.append((score, hyp))\n",
        "            if len(self) > self.n_hyp:\n",
        "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n",
        "                del self.hyp[sorted_scores[0][1]]\n",
        "                self.worst_score = sorted_scores[1][0]\n",
        "            else:\n",
        "                self.worst_score = min(score, self.worst_score)\n",
        "\n",
        "    def is_done(self, best_sum_logprobs):\n",
        "        \"\"\"\n",
        "        If there are enough hypotheses and that none of the hypotheses being generated\n",
        "        can become better than the worst one in the heap, then we are done with this sentence.\n",
        "        \"\"\"\n",
        "        if len(self) < self.n_hyp:\n",
        "            return False\n",
        "        elif self.early_stopping:\n",
        "            return True\n",
        "        else:\n",
        "            return self.worst_score >= best_sum_logprobs / self.max_len**self.length_penalty"
      ],
      "metadata": {
        "id": "R_LJL4Dj5Xib"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_degree = 6          # max degree of the equations in the system\n",
        "int_base = 1000         # int base for the tokenizer\n",
        "\n",
        "operators = ['+', '-', '*', '/', '^', 'sqrt', 'exp', 'ln', 'sin', 'cos', 'tan', 'asin', 'acos', 'atan', 'Abs']\n",
        "variables = [f\"x{i}\" for i in range(2 * max_degree)]\n",
        "constants = [\"pi\", \"E\"]\n",
        "symbols = [\"I\", \"INT+\", \"INT-\", \"FLOAT+\", \"FLOAT-\", \".\", \"10^\"]\n",
        "elements = [str(i) for i in range(max(10, int_base))]\n",
        "SPECIAL_WORDS = [\"<s>\", \"</s>\", \"<pad>\", \"(\", \")\"] + [f\"<SPECIAL_{i}>\" for i in range(10)]\n",
        "func_separator = \"<SPECIAL_3>\"\n",
        "\n",
        "words = SPECIAL_WORDS + constants + variables + operators + symbols + elements\n",
        "id2word = {i: s for i, s in enumerate(words)} # tokenizer dictionary\n",
        "word2id = {s: i for i, s in id2word.items()}\n",
        "n_words = len(words)\n",
        "eos_index = 0\n",
        "pad_index = 1"
      ],
      "metadata": {
        "id": "6-TN8yLQaiPs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Helper class for creating datasets.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_workers, path, word2id, train, max_len=1024, max_output_len=512, reload_size=-1, size=None):\n",
        "        super().__init__()\n",
        "        self.num_workers = num_workers\n",
        "        self.path = path\n",
        "        self.train = train\n",
        "        self.count = 0\n",
        "        assert size is None\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.max_output_len = max_output_len\n",
        "        self.word2id = word2id\n",
        "\n",
        "        self.func_separator = \"<SPECIAL_3>\"\n",
        "        self.eos_index = 0\n",
        "        self.pad_index = 1\n",
        "\n",
        "        # reloading from file\n",
        "        if path is not None:\n",
        "            assert os.path.isfile(path)\n",
        "            print(f\"Loading data from {path} ...\")\n",
        "            with io.open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "                # either reload the entire file, or the first N lines (for the training set)\n",
        "                if not train:\n",
        "                    lines = [line.rstrip().split(\"|\") for line in f]\n",
        "                else:\n",
        "                    lines = []\n",
        "                    for i, line in enumerate(f):\n",
        "                        if i == reload_size:\n",
        "                            break\n",
        "                        lines.append(line.rstrip().split(\"|\"))\n",
        "            self.data = [xy.split(\"\\t\") for _, xy in lines]\n",
        "            self.data = [xy for xy in self.data if len(xy) == 2]\n",
        "            print(f\"Loaded {len(self.data)} equations from the disk.\")\n",
        "\n",
        "        # dataset size: infinite iterator for train, finite for validation (default of 5000 if no file provided)\n",
        "        if self.train:\n",
        "            self.size = 1 << 60\n",
        "        elif size is None:\n",
        "            self.size = 5000 if path is None else len(self.data)\n",
        "        else:\n",
        "            assert size > 0\n",
        "            self.size = size\n",
        "\n",
        "    def batch_sequences(self, sequences):\n",
        "        \"\"\"\n",
        "        Take as input a list of n sequences (torch.LongTensor vectors) and return\n",
        "        a tensor of size (slen, n) where slen is the length of the longest\n",
        "        sentence, and a vector lengths containing the length of each sentence.\n",
        "        \"\"\"\n",
        "        lengths = torch.LongTensor([len(s) + 2 for s in sequences])\n",
        "        sent = torch.LongTensor(lengths.max().item(), lengths.size(0)).fill_(self.pad_index)\n",
        "        assert lengths.min().item() > 2\n",
        "\n",
        "        sent[0] = self.eos_index\n",
        "        for i, s in enumerate(sequences):\n",
        "            sent[1 : lengths[i] - 1, i].copy_(s)\n",
        "            sent[lengths[i] - 1, i] = self.eos_index\n",
        "\n",
        "        return sent, lengths\n",
        "\n",
        "    def collate_fn(self, elements):\n",
        "        \"\"\"\n",
        "        Collate samples into a batch.\n",
        "        \"\"\"\n",
        "        x, y = zip(*elements)\n",
        "        nb_eqs = [seq.count(self.func_separator) for seq in x]\n",
        "        x = [torch.LongTensor([self.word2id[w] for w in seq]) for seq in x]\n",
        "        y = [torch.LongTensor([self.word2id[w] for w in seq]) for seq in y]\n",
        "        x, x_len = self.batch_sequences(x)\n",
        "        y, y_len = self.batch_sequences(y)\n",
        "        return (x, x_len), (y, y_len), torch.LongTensor(nb_eqs)\n",
        "\n",
        "    def get_worker_id(self):\n",
        "        \"\"\"\n",
        "        Get worker ID.\n",
        "        \"\"\"\n",
        "        if not self.train:\n",
        "            return 0\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        assert (worker_info is None) == (self.num_workers == 0)\n",
        "        return 0 if worker_info is None else worker_info.id\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return dataset size.\n",
        "        \"\"\"\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Return a training  by reading it from a file.\n",
        "        \"\"\"\n",
        "        return self.read_sample(index)\n",
        "\n",
        "    def read_sample(self, index):\n",
        "        \"\"\"\n",
        "        Read a sample.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            if self.train:\n",
        "                index = random.randint(0, len(self.data)-1)\n",
        "            x, y = self.data[index]\n",
        "            x = x.split()\n",
        "            y = y.split()\n",
        "            if (self.max_len > 0 and len(x) >= self.max_len) or (self.max_output_len > 0 and len(y) >= self.max_output_len):\n",
        "                index += 1\n",
        "                continue\n",
        "            return x, y"
      ],
      "metadata": {
        "id": "SWL5Zn963yhh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_iterator(data_path, num_workers, batch_size, word2id):\n",
        "    \"\"\"\n",
        "    Create a training dataset.\n",
        "    \"\"\"\n",
        "    print(f\"Creating train iterator...\")\n",
        "\n",
        "    if num_workers is None:\n",
        "        num_workers = min(4, os.cpu_count() or 1)\n",
        "\n",
        "    dataset = EnvDataset(path=data_path, num_workers=num_workers, word2id=word2id, train=True)\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        timeout=(0 if num_workers == 0 else 86400),\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=False,\n",
        "        collate_fn=dataset.collate_fn,\n",
        "    )"
      ],
      "metadata": {
        "id": "LPtK85U226Iv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(dataloader):\n",
        "        \"\"\"\n",
        "        Return a training batch for a specific task.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            batch = next(dataloader)\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                \"An unknown exception occurred when fetching batch. \"\n",
        "            )\n",
        "            raise\n",
        "        return batch"
      ],
      "metadata": {
        "id": "cVSk-Oia2EXz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_cuda(*args):\n",
        "    \"\"\"\n",
        "    Move tensors to CUDA.\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return args\n",
        "    return [None if x is None else x.cuda() for x in args]\n",
        "\n",
        "def enc_dec_step(encoder, decoder, dataloader, optimizer, clip_grad_norm=5):\n",
        "    \"\"\"\n",
        "    Encoding / decoding step.\n",
        "    \"\"\"\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # batch\n",
        "    (x1, len1), (x2, len2), nb_ops = get_batch(dataloader)\n",
        "\n",
        "    # cuda\n",
        "    x1, len1, x2, len2 = to_cuda(x1, len1, x2, len2)\n",
        "\n",
        "    # target words to predict\n",
        "    alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)\n",
        "    pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word\n",
        "\n",
        "    y = x2[1:].masked_select(pred_mask[:-1])\n",
        "\n",
        "    assert len(y) == (len2 - 1).sum().item()\n",
        "\n",
        "    # forward / loss\n",
        "    encoded = encoder(\"fwd\", x=x1, lengths=len1, causal=False)\n",
        "    decoded = decoder(\"fwd\", x=x2, lengths=len2, causal=True, src_enc=encoded.transpose(0, 1), src_len=len1)\n",
        "    _, loss = decoder(\"predict\", tensor=decoded, pred_mask=pred_mask, y=y, get_scores=False)\n",
        "\n",
        "    # check NaN\n",
        "    if (loss != loss).data.any():\n",
        "        print(\"NaN detected\")\n",
        "\n",
        "    parameters = {\n",
        "        k: p for v in [encoder, decoder] for k, p in v.named_parameters() if p.requires_grad\n",
        "    }\n",
        "    model_params = list(parameters.values())\n",
        "\n",
        "    # regular optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    if clip_grad_norm > 0:\n",
        "        clip_grad_norm_(model_params, clip_grad_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "azFI-YOT2M47"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "max_epoch = 8       # how many epochs will the model train\n",
        "epoch = 0           # current epoch\n",
        "epoch_size = 600    # number of equations in an epoch\n",
        "n_total_iter = 0    # total number of passed iterations\n",
        "\n",
        "parser = get_parser()\n",
        "\n",
        "parser.add_argument(\"--ul_alpha\", type=float, default=0.0)\n",
        "parser.add_argument(\"--ul_topp\", type=float, default=0.9)\n",
        "parser.add_argument(\"--ul_topk\", type=int, default=20)\n",
        "parser.add_argument(\"--ul_temp\", type=float, default=0.8)\n",
        "\n",
        "parser.add_argument(\"--contrastive_alpha\", type=float, default=0.0)\n",
        "parser.add_argument(\"--contrastive_temp\", type=float, default=0.1)\n",
        "\n",
        "params = parser.parse_args([])    # setup of all transformer parameters\n",
        "\n",
        "batch_size = 16\n",
        "data_path = \"Data.txt\"\n",
        "dump_path = \"\" # path for saving the model\n",
        "model_name = \"Lyapunov\"\n",
        "dataloader = iter(create_train_iterator(data_path=data_path, num_workers=None, batch_size=batch_size, word2id=word2id))\n",
        "\n",
        "encoder = TransformerModel(params, id2word=id2word, is_encoder=True, with_output=False)\n",
        "decoder = TransformerModel(params, id2word=id2word, is_encoder=False, with_output=True)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
        "\n",
        "last_time = time.time()\n",
        "\n",
        "for _ in range(max_epoch):\n",
        "\n",
        "    print(\"============ Starting epoch %i ... ============\" % epoch)\n",
        "\n",
        "    n_equations = 0\n",
        "    n_iter = 0\n",
        "\n",
        "    while n_equations < epoch_size:\n",
        "\n",
        "        # training steps\n",
        "        loss = enc_dec_step(encoder, decoder, dataloader, optimizer)\n",
        "        n_equations += batch_size\n",
        "        n_iter += 1\n",
        "        n_total_iter += 1\n",
        "\n",
        "        if n_total_iter % 10 == 0:\n",
        "          new_time = time.time()\n",
        "          diff = new_time - last_time\n",
        "          print('Training speed is ', 20/diff, ' iter/s. The loss in ', n_iter, '. iteration: ', loss.item())\n",
        "          last_time = new_time\n",
        "\n",
        "    print(\"============ End of epoch %i ============\" % epoch)\n",
        "\n",
        "    # end of epoch\n",
        "\n",
        "    \"\"\"\n",
        "    Save the model / checkpoints.\n",
        "    \"\"\"\n",
        "    path = os.path.join(dump_path, \"%s.pth\" % model_name)\n",
        "    print(\"Saving %s to %s ...\" % (model_name, path))\n",
        "\n",
        "    data = {\n",
        "        \"epoch\": epoch,\n",
        "        \"n_total_iter\": n_total_iter,\n",
        "        \"loss\": loss.item(),\n",
        "    }\n",
        "\n",
        "    print(f\"Saving encoder parameters ...\")\n",
        "    data[\"encoder\"] = encoder.state_dict()\n",
        "    print(f\"Saving decoder parameters ...\")\n",
        "    data[\"decoder\"] = decoder.state_dict()\n",
        "\n",
        "    print(\"Saving optimizer ...\")\n",
        "    data[\"optimizer\"] = optimizer.state_dict()\n",
        "\n",
        "    torch.save(data, path)\n",
        "\n",
        "    epoch += 1"
      ],
      "metadata": {
        "id": "NNH6kg6hNjlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e276fe13-e1a9-4edb-cb5f-3856e62b4aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating train iterator...\n",
            "Loading data from Data.txt ...\n",
            "Loaded 600 equations from the disk.\n",
            "============ Starting epoch 0 ... ============\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation\n",
        "\n",
        "!git clone https://github.com/facebookresearch/Lyapunov/ # optim.py will signal an error, just comment problematic lines\n",
        "!python Lyapunov/train.py --dump_path \"\" --export_data false --cpu true --reload_data \"ode_lyapunov,Lyapunov/benchmarks/BPoly,Lyapunov/benchmarks/FBarr,Lyapunov/benchmarks/FLyap,Lyapunov/benchmarks/FSOSTOOL\" --env_base_seed -1  --num_workers 1 --eval_only true --reload_model \"Lyapunov.pth\""
      ],
      "metadata": {
        "id": "3fqHhf-Goo-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sympy as sp\n",
        "from collections import OrderedDict\n",
        "\n",
        "class MathTokenizer:    # tokenizer implementation from the paper\n",
        "    def __init__(self, max_degree, int_base):\n",
        "        self.max_degree = max_degree\n",
        "        self.int_base = int_base\n",
        "\n",
        "        self.operators = {\n",
        "            \"+\": 2, \"-\": 2, \"*\": 2, \"/\": 2, \"^\": 2,\n",
        "            \"sqrt\": 1, \"exp\": 1, \"ln\": 1,\n",
        "            \"sin\": 1, \"cos\": 1, \"tan\": 1,\n",
        "            \"asin\": 1, \"acos\": 1, \"atan\": 1,\n",
        "            \"Abs\": 1\n",
        "        }\n",
        "\n",
        "        self.variables = OrderedDict({f\"x{i}\": sp.Symbol(f\"x{i}\") for i in range(2 * self.max_degree)})\n",
        "        self.constants = [\"pi\", \"E\"]\n",
        "        self.symbols = [\"I\", \"INT+\", \"INT-\", \"FLOAT+\", \"FLOAT-\", \".\", \"10^\"]\n",
        "        self.elements = [str(i) for i in range(max(10, self.int_base))]\n",
        "\n",
        "        SPECIAL_WORDS = [\"<s>\", \"</s>\", \"<pad>\", \"(\", \")\"]\n",
        "        SPECIAL_WORDS += [f\"<SPECIAL_{i}>\" for i in range(10)]\n",
        "\n",
        "        self.words = SPECIAL_WORDS + self.constants + list(self.variables.keys()) + list(self.operators.keys()) + self.symbols + self.elements\n",
        "        self.id2word = {i: s for i, s in enumerate(self.words)}\n",
        "        self.word2id = {s: i for i, s in self.id2word.items()}\n",
        "        self.n_words = len(self.words)\n",
        "        self.eos_index = eos_index = 0\n",
        "        self.pad_index = pad_index = 1\n",
        "\n",
        "    def parse_expression(self, expr):\n",
        "        tokens = re.findall(r'sqrt|exp|ln|sin|cos|tan|asin|acos|atan|Abs|[+\\-*/^()]|x\\d+|pi|E|\\d+\\.?\\d*', expr)\n",
        "        pos = 0\n",
        "\n",
        "        def peek():\n",
        "            return tokens[pos] if pos < len(tokens) else None\n",
        "\n",
        "        def consume(expected=None):\n",
        "            nonlocal pos\n",
        "            token = tokens[pos]\n",
        "            if expected and token != expected:\n",
        "                raise ValueError(f\"Expected {expected} but got {token}\")\n",
        "            pos += 1\n",
        "            return token\n",
        "\n",
        "        class TreeNode:\n",
        "            def __init__(self, value):\n",
        "                self.value = value\n",
        "                self.children = []\n",
        "\n",
        "            def add_child(self, child):\n",
        "                self.children.append(child)\n",
        "\n",
        "            def __repr__(self):\n",
        "                return f\"TreeNode({self.value}, {self.children})\"\n",
        "\n",
        "        def parse_primary():\n",
        "            token = peek()\n",
        "            if token is None:\n",
        "                raise ValueError(\"Unexpected end of expression\")\n",
        "            if token == '(':\n",
        "                consume('(')\n",
        "                node = parse_expr()\n",
        "                consume(')')\n",
        "                return node\n",
        "            elif token in self.operators and self.operators[token] == 1:\n",
        "                func = consume()\n",
        "                node = TreeNode(func)\n",
        "                consume('(')\n",
        "                node.add_child(parse_expr())\n",
        "                consume(')')\n",
        "                return node\n",
        "            else:\n",
        "                return TreeNode(consume())\n",
        "\n",
        "        def parse_factor():\n",
        "            return parse_primary()\n",
        "\n",
        "        def parse_term():\n",
        "            node = parse_factor()\n",
        "            while peek() in ('*', '/'):\n",
        "                op = consume()\n",
        "                new_node = TreeNode(op)\n",
        "                new_node.add_child(node)\n",
        "                new_node.add_child(parse_factor())\n",
        "                node = new_node\n",
        "            return node\n",
        "\n",
        "        def parse_expr():\n",
        "            node = parse_term()\n",
        "            while peek() in ('+', '-'):\n",
        "                op = consume()\n",
        "                new_node = TreeNode(op)\n",
        "                new_node.add_child(node)\n",
        "                new_node.add_child(parse_term())\n",
        "                node = new_node\n",
        "            return node\n",
        "\n",
        "        return parse_expr()\n",
        "\n",
        "    def enumerate_tree(self, tree):\n",
        "        if not tree:\n",
        "            return []\n",
        "        result = [tree.value]\n",
        "        for child in tree.children:\n",
        "            result.extend(self.enumerate_tree(child))\n",
        "        return result\n",
        "\n",
        "    def tokenize_integer(self, n):\n",
        "        if n == 0:\n",
        "            return ['0']\n",
        "        sign = '-' if n < 0 else None\n",
        "        n = abs(n)\n",
        "        tokens = []\n",
        "        while n > 0:\n",
        "            remainder = n % 1000\n",
        "            tokens.append(str(remainder))\n",
        "            n //= 1000\n",
        "        tokens = tokens[::-1]\n",
        "        if sign:\n",
        "            tokens = [sign] + tokens\n",
        "        return tokens\n",
        "\n",
        "    def tokenize_real(self, x):\n",
        "        x = abs(x)\n",
        "        s = str(x)\n",
        "        if '.' in s:\n",
        "            integer_part, fractional_part = s.split('.')\n",
        "            fractional_part = fractional_part.rstrip('0')\n",
        "            if fractional_part == '':\n",
        "                mantissa = integer_part\n",
        "                exponent = 0\n",
        "            else:\n",
        "                mantissa = integer_part + fractional_part\n",
        "                exponent = -len(fractional_part)\n",
        "        else:\n",
        "            mantissa = s\n",
        "            exponent = 0\n",
        "        return self.tokenize_integer(int(mantissa)) + ['10^'] + self.tokenize_integer(exponent)\n",
        "\n",
        "    def tokenize_expression(self, expression):\n",
        "        tokens = []\n",
        "        for token in expression:\n",
        "            if token in self.words:\n",
        "                tokens.append(token)\n",
        "            elif re.match(r'^-?\\d+$', token):\n",
        "                tokens.extend(self.tokenize_integer(int(token)))\n",
        "            elif re.match(r'^-?\\d+\\.\\d*$', token):\n",
        "                tokens.extend(self.tokenize_real(float(token)))\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown token: {token}\")\n",
        "        return tokens\n",
        "\n",
        "    def tokenize(self, expressions):\n",
        "        tokenized_expressions = []\n",
        "        for expression in expressions:\n",
        "            tree = self.parse_expression(expression)\n",
        "            enumerated = self.enumerate_tree(tree)\n",
        "            tokens = self.tokenize_expression(enumerated)\n",
        "            tokenized_expressions.extend(tokens + [\"<SPECIAL_3>\"])\n",
        "        return tokenized_expressions[:-1]\n",
        "\n",
        "    def encode(self, tokenized_expressions):\n",
        "      return [self.word2id[id] for id in tokenized_expressions]\n",
        "\n",
        "tokenizer = MathTokenizer(int_base=1024, max_degree=3)\n",
        "expressions = [\"cos(2.1 * x0) * (x1 + 2)\", \"sin(3 * x1 + 2)\"]\n",
        "print(expressions)\n",
        "tokenized_expr = tokenizer.tokenize(expressions)\n",
        "tokenized = tokenizer.encode(tokenized_expr)\n",
        "print(tokenized_expr)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "id": "c7H8NLVW4sGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5705ec4b-b629-4ee8-c5fa-f94db5ff7475"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cos(2.1 * x0) * (x1 + 2)', 'sin(3 * x1 + 2)']\n",
            "['*', 'cos', '*', '21', '10^', '-', '1', 'x0', '+', 'x1', '2', '<SPECIAL_3>', 'sin', '+', '*', '3', 'x1', '2']\n",
            "[25, 32, 25, 66, 44, 24, 46, 17, 23, 18, 47, 8, 31, 23, 25, 48, 18, 47]\n"
          ]
        }
      ]
    }
  ]
}