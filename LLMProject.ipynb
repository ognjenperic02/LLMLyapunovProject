{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeUonic26WPe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch import optim\n",
        "import os\n",
        "import io\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import numpy\n",
        "\n",
        "from ode import ODEEnvironment\n",
        "from train import get_parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingCust(nn.Module):\n",
        "    def __init__(self, num_embed, embed_dim, pad_idx=None):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embed, embed_dim, padding_idx=pad_idx)\n",
        "        nn.init.normal_(self.embedding.weight, mean=0, std=embed_dim**-0.5)\n",
        "        if pad_idx is not None:\n",
        "            nn.init.constant_(self.embedding.weight[pad_idx], 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "def get_masks(seq_len, lengths, causal_att):\n",
        "    \"\"\"\n",
        "    Create masks for hidden states and attention.\n",
        "    \"\"\"\n",
        "    assert lengths.max().item() <= seq_len\n",
        "    batch_size = lengths.size(0)\n",
        "    seq_range = torch.arange(seq_len, dtype=torch.long, device=lengths.device)\n",
        "    mask = seq_range < lengths[:, None]\n",
        "\n",
        "    if causal_att:\n",
        "        attn_mask = seq_range[None, None, :].repeat(batch_size, seq_len, 1) <= seq_range[None, :, None]\n",
        "    else:\n",
        "        attn_mask = mask\n",
        "\n",
        "    return mask, attn_mask\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    _id_counter = 0\n",
        "\n",
        "    def __init__(self, n_heads, embed_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layer_id = MultiHeadAttention._id_counter\n",
        "        MultiHeadAttention._id_counter += 1\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "        assert self.embed_dim % self.n_heads == 0\n",
        "\n",
        "        self.q_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_lin = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_lin = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, mask, kv=None, use_cache=False):\n",
        "        \"\"\"\n",
        "        Compute self-attention or cross-attention.\n",
        "        \"\"\"\n",
        "        batch_size, q_len, _ = query.size()\n",
        "        k_len = q_len if kv is None else kv.size(1)\n",
        "\n",
        "        head_dim = self.embed_dim // self.n_heads\n",
        "        reshaped_mask = (batch_size, 1, q_len, k_len) if mask.dim() == 3 else (batch_size, 1, 1, k_len)\n",
        "\n",
        "        q = self.q_lin(query).view(batch_size, q_len, self.n_heads, head_dim).transpose(1, 2)\n",
        "        if kv is None:\n",
        "            k = self.k_lin(query).view(batch_size, q_len, self.n_heads, head_dim).transpose(1, 2)\n",
        "            v = self.v_lin(query).view(batch_size, q_len, self.n_heads, head_dim).transpose(1, 2)\n",
        "        else:\n",
        "            k = self.k_lin(kv).view(batch_size, k_len, self.n_heads, head_dim).transpose(1, 2)\n",
        "            v = self.v_lin(kv).view(batch_size, k_len, self.n_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        q = q / math.sqrt(head_dim)\n",
        "        att_scores = torch.matmul(q, k.transpose(2, 3))\n",
        "        mask = (mask == 0).view(reshaped_mask).expand_as(att_scores)\n",
        "        att_scores.masked_fill_(mask, -float(\"inf\"))\n",
        "\n",
        "        att_weights = F.softmax(att_scores.float(), dim=-1).type_as(att_scores)\n",
        "        att_weights = F.dropout(att_weights, p=self.dropout, training=self.training)\n",
        "        context = torch.matmul(att_weights, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, q_len, self.embed_dim)\n",
        "\n",
        "        return self.out_lin(context)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lin2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.lin2(x)\n",
        "        return F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, half_prec, is_encoder, num_words, eos_idx, pad_idx, id2word, n_max_positions=4096,\n",
        "                 embed_dim=512, n_heads=8, num_enc_layers=6, num_dec_layers=6,\n",
        "                 dropout=0.1, att_dropout=0.1, max_src_len=512, with_output=True):\n",
        "        \"\"\"\n",
        "        Transformer model for encoding or decoding.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.dtype = torch.half if half_prec else torch.float\n",
        "        self.is_encoder = is_encoder\n",
        "        self.num_words = num_words\n",
        "        self.eos_idx = eos_idx\n",
        "        self.pad_idx = pad_idx\n",
        "        self.id2word = id2word\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = embed_dim * 4\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = num_enc_layers if is_encoder else num_dec_layers\n",
        "        self.dropout = dropout\n",
        "        self.att_dropout = att_dropout\n",
        "        self.max_src_len = max_src_len\n",
        "\n",
        "        self.position_embeddings = EmbeddingCust(n_max_positions, embed_dim)\n",
        "        self.embeddings = EmbeddingCust(num_words, embed_dim, pad_idx=pad_idx)\n",
        "        self.emb_layer_norm = nn.LayerNorm(embed_dim, eps=1e-12)\n",
        "\n",
        "        self.mhas = nn.ModuleList()\n",
        "        self.layer_norms1 = nn.ModuleList()\n",
        "        self.ffns = nn.ModuleList()\n",
        "        self.layer_norms2 = nn.ModuleList()\n",
        "        self.layer_norms_dec = nn.ModuleList() if not is_encoder else None\n",
        "        self.encoder_atts = nn.ModuleList() if not is_encoder else None\n",
        "\n",
        "        for _ in range(self.n_layers):\n",
        "            self.mhas.append(MultiHeadAttention(n_heads, embed_dim, dropout=att_dropout))\n",
        "            self.layer_norms1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
        "            self.ffns.append(FeedForward(embed_dim, self.hidden_dim, embed_dim, dropout=dropout))\n",
        "            self.layer_norms2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
        "\n",
        "            if not is_encoder:\n",
        "                self.layer_norms_dec.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
        "                self.encoder_atts.append(MultiHeadAttention(n_heads, embed_dim, dropout=att_dropout))\n",
        "\n",
        "        self.cache = None\n",
        "        self.final_lin = nn.Linear(embed_dim, num_words) if with_output else None\n",
        "\n",
        "    def forward(self, mode, **kwargs):\n",
        "        \"\"\"\n",
        "        Handles multiple forward modes.\n",
        "        \"\"\"\n",
        "        if mode == \"fwd\":\n",
        "            return self.fwd(**kwargs)\n",
        "        elif mode == \"predict\":\n",
        "            return self.predict(**kwargs)\n",
        "        else:\n",
        "            raise Exception(\"Unknown mode: %s\" % mode)\n",
        "\n",
        "    def fwd(self, x, lengths, causal_att, src_enc=None, src_len=None, positions=None, use_cache=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            `x` LongTensor(slen, batch_size), token indices\n",
        "            `lengths` LongTensor(batch_size), sentence lengths\n",
        "            `causal_att` Boolean, enables causal attention\n",
        "            `positions` LongTensor(slen, batch_size), token positions\n",
        "        \"\"\"\n",
        "        slen, batch_size = x.size()\n",
        "        assert lengths.size(0) == batch_size\n",
        "        assert lengths.max().item() <= slen\n",
        "        x = x.transpose(0, 1)  # Set batch size as first dimension\n",
        "        assert (src_enc is None) == (src_len is None)\n",
        "        if src_enc is not None:\n",
        "            assert self.is_encoder\n",
        "            assert src_enc.size(0) == batch_size\n",
        "        assert not (use_cache and self.cache is None)\n",
        "\n",
        "        # Generate masks\n",
        "        mask, attn_mask = get_masks(slen, lengths, causal_att)\n",
        "        if self.is_encoder and src_enc is not None:\n",
        "            if self.max_src_len > 0:\n",
        "                src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < torch.clamp(src_len[:, None], max=self.max_src_len)\n",
        "            else:\n",
        "                src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]\n",
        "\n",
        "        # Compute positions\n",
        "        if positions is None:\n",
        "            positions = x.new(slen).long()\n",
        "            positions = torch.arange(slen, out=positions).unsqueeze(0)\n",
        "        else:\n",
        "            assert positions.size() == (slen, batch_size)\n",
        "            positions = positions.transpose(0, 1)\n",
        "\n",
        "        # Handle cached elements\n",
        "        if use_cache:\n",
        "            _slen = slen - self.cache[\"slen\"]\n",
        "            x = x[:, -_slen:]\n",
        "            positions = positions[:, -_slen:]\n",
        "            mask = mask[:, -_slen:]\n",
        "            attn_mask = attn_mask[:, -_slen:]\n",
        "\n",
        "        # Compute embeddings\n",
        "        tensor = self.embeddings(x) + self.position_embeddings(positions).expand_as(x)\n",
        "        tensor = self.emb_layer_norm(tensor)\n",
        "        tensor = F.dropout(tensor, p=self.dropout, training=self.training)\n",
        "        tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "\n",
        "        # Transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "            # Multi-head self-attention\n",
        "            self.mhas[i].cache = self.cache\n",
        "            attn = self.mhas[i](tensor, attn_mask, use_cache=use_cache)\n",
        "            attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "            tensor = tensor + attn\n",
        "            tensor = self.layer_norms1[i](tensor)\n",
        "\n",
        "            # Cross-attention (only for decoder)\n",
        "            if self.is_encoder and src_enc is not None:\n",
        "                self.encoder_atts[i].cache = self.cache\n",
        "                attn = self.encoder_atts[i](tensor, src_mask, kv=src_enc, use_cache=use_cache)\n",
        "                attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "                tensor = tensor + attn\n",
        "                tensor = self.layer_norms_dec[i](tensor)\n",
        "\n",
        "            # Feedforward network\n",
        "            tensor = tensor + self.ffns[i](tensor)\n",
        "            tensor = self.layer_norms2[i](tensor)\n",
        "\n",
        "            tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "\n",
        "        # Update cache\n",
        "        if use_cache:\n",
        "            self.cache[\"slen\"] += tensor.size(1)\n",
        "\n",
        "        return tensor.transpose(0, 1)\n",
        "\n",
        "    def predict(self, tensor, pred_mask, y, get_scores):\n",
        "        \"\"\"\n",
        "        Compute scores and/or loss from hidden states.\n",
        "        \"\"\"\n",
        "        x = tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.embed_dim)\n",
        "        assert (y == self.pad_idx).sum().item() == 0\n",
        "        scores = self.final_lin(x).view(-1, self.num_words)\n",
        "        loss = F.cross_entropy(scores.float(), y, reduction=\"mean\")\n",
        "        return scores, loss\n",
        "\n",
        "    def generate(self, src_enc, src_len, max_len=200, sample_temp=None):\n",
        "        \"\"\"\n",
        "        Generate a sentence given the initial input.\n",
        "        `x`:\n",
        "            - LongTensor(batch_size, slen)\n",
        "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
        "                <EOS> W1 W2 W3 W4 <EOS>\n",
        "        `lengths`:\n",
        "            - LongTensor(batch_size) [5, 6]\n",
        "        `positions`:\n",
        "            - False for regular position encoding (LM)\n",
        "            - True to reset positions during generation (MT)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the batch size and ensure the source encoding matches\n",
        "        batch_size = len(src_len)\n",
        "        assert src_enc.size(0) == batch_size\n",
        "\n",
        "        # Prepare the tensor for generated sentences\n",
        "        generated = src_len.new(max_len, batch_size)  # upcoming output\n",
        "        generated.fill_(self.pad_idx)  # initialize with <PAD>\n",
        "        generated[0].fill_(self.eos_idx)  # start with <EOS> as <BOS>\n",
        "\n",
        "        # Initialize positions\n",
        "        positions = src_len.new(max_len).long()\n",
        "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand(max_len, batch_size)\n",
        "\n",
        "        # Set up generation length tracking\n",
        "        cur_len = 1\n",
        "        gen_len = src_len.clone().fill_(1)\n",
        "        unfinished_sents = src_len.clone().fill_(1)\n",
        "\n",
        "        # Cache computation states\n",
        "        self.cache = {\"slen\": 0}\n",
        "\n",
        "        # Start the generation loop\n",
        "        while cur_len < max_len:\n",
        "\n",
        "            # Compute word scores\n",
        "            tensor = self.forward(\n",
        "                \"fwd\",\n",
        "                x=generated[:cur_len],\n",
        "                lengths=gen_len,\n",
        "                positions=positions[:cur_len],\n",
        "                causal_att=True,\n",
        "                src_enc=src_enc,\n",
        "                src_len=src_len,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            assert tensor.size() == (1, batch_size, self.embed_dim)\n",
        "            tensor = tensor.data[-1, :, :]  # (batch_size, embed_dim)\n",
        "            scores = self.final_lin(tensor)  # (batch_size, num_words)\n",
        "\n",
        "            # Select the next words: sample or greedy\n",
        "            if sample_temp is None:\n",
        "                next_words = torch.topk(scores, 1)[1].squeeze(1)\n",
        "            else:\n",
        "                next_words = torch.multinomial(F.softmax(scores.float() / sample_temp, dim=1), 1).squeeze(1)\n",
        "            assert next_words.size() == (batch_size,)\n",
        "\n",
        "            # Update generations, lengths, unfinished sentences, and current length\n",
        "            generated[cur_len] = next_words * unfinished_sents + self.pad_idx * (1 - unfinished_sents)\n",
        "            gen_len.add_(unfinished_sents)\n",
        "            unfinished_sents.mul_(next_words.ne(self.eos_idx).long())\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            # Stop if each sentence has an </s> or max length is reached\n",
        "            if unfinished_sents.max() == 0:\n",
        "                break\n",
        "\n",
        "        # Add <EOS> to unfinished sentences if max length is reached\n",
        "        if cur_len == max_len:\n",
        "            generated[-1].masked_fill_(unfinished_sents.byte(), self.eos_idx)\n",
        "\n",
        "        # Sanity check\n",
        "        assert (generated == self.eos_idx).sum() == 2 * batch_size\n",
        "\n",
        "        return generated[:cur_len], gen_len\n",
        "\n",
        "    def generate_beam(self, src_enc, src_len, beam_size, len_penalty, early_stopping, max_len=200):\n",
        "        \"\"\"\n",
        "        Generate a sequence using beam search decoding.\n",
        "\n",
        "        `src_enc`:\n",
        "            - LongTensor(batch_size, sequence_length)\n",
        "            Contains source sentences, with <EOS> tokens at the beginning and end.\n",
        "\n",
        "        `src_len`:\n",
        "            - LongTensor(batch_size)\n",
        "            Represents the lengths of the source sentences.\n",
        "\n",
        "        `beam_size`:\n",
        "            - Integer\n",
        "            Specifies the number of hypotheses to maintain at each decoding step.\n",
        "\n",
        "        `len_penalty`:\n",
        "            - Float\n",
        "            A penalty factor for the length of the decoded sequences.\n",
        "\n",
        "        `early_stopping`:\n",
        "            - Boolean\n",
        "            If True, decoding stops once the best hypothesis has been found.\n",
        "\n",
        "        `max_len`:\n",
        "            - Integer, default=200\n",
        "            The maximum length of the generated sequence.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure input consistency\n",
        "        assert src_enc.size(0) == src_len.size(0), \"Mismatch between batch size and source lengths\"\n",
        "        assert beam_size >= 1, \"Beam size must be greater than or equal to 1\"\n",
        "\n",
        "        # Extract batch size and number of words in the vocabulary\n",
        "        batch_size = len(src_len)\n",
        "        num_words = self.num_words\n",
        "\n",
        "        # Expand source encoder and lengths to match beam size\n",
        "        src_enc = src_enc.unsqueeze(1).expand((batch_size, beam_size) + src_enc.shape[1:]).contiguous().view((batch_size * beam_size,) + src_enc.shape[1:])\n",
        "        src_len = src_len.unsqueeze(1).expand(batch_size, beam_size).contiguous().view(-1)\n",
        "\n",
        "        # Initialize output tensor and fill it with padding tokens\n",
        "        generated = src_len.new(max_len, batch_size * beam_size)\n",
        "        generated.fill_(self.pad_idx)\n",
        "        generated[0].fill_(self.eos_idx)  # Start sequence with <EOS>\n",
        "\n",
        "        # List to store generated hypotheses for each sentence\n",
        "        generated_hyps = [BeamHypotheses(beam_size, max_len, len_penalty, early_stopping) for _ in range(batch_size)]\n",
        "\n",
        "        # Prepare positions tensor for the sequence\n",
        "        positions = src_len.new(max_len).long()\n",
        "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand_as(generated)\n",
        "\n",
        "        # Initialize beam scores and set the first beam score to 0\n",
        "        beam_scores = src_enc.new(batch_size, beam_size).float().fill_(0)\n",
        "        beam_scores[:, 1:] = -1e9  # Prevent the second beam from being selected initially\n",
        "        beam_scores = beam_scores.view(-1)\n",
        "\n",
        "        # Initialize current length to 1 (since we start with <EOS>)\n",
        "        cur_len = 1\n",
        "\n",
        "        # Cache to store computed states\n",
        "        self.cache = {\"slen\": 0}\n",
        "\n",
        "        # Initialize 'done' flag for each sentence\n",
        "        done = [False for _ in range(batch_size)]\n",
        "\n",
        "        while cur_len < max_len:\n",
        "            # Compute word scores using the model's forward pass\n",
        "            tensor = self.forward(\n",
        "                \"fwd\",\n",
        "                x=generated[:cur_len],\n",
        "                lengths=src_len.new(batch_size * beam_size).fill_(cur_len),\n",
        "                positions=positions[:cur_len],\n",
        "                causal_att=True,\n",
        "                src_enc=src_enc,\n",
        "                src_len=src_len,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            assert tensor.size() == (1, batch_size * beam_size, self.embed_dim)\n",
        "            tensor = tensor.data[-1, :, :]  # Extract the relevant tensor from the last time step\n",
        "            scores = self.final_lin(tensor)  # Apply final linear transformation to obtain scores\n",
        "            scores = F.log_softmax(scores.float(), dim=-1)  # Apply log softmax to get probabilities\n",
        "\n",
        "            # Calculate total scores by adding beam scores to the word scores\n",
        "            _scores = scores + beam_scores[:, None].expand_as(scores)\n",
        "            _scores = _scores.view(batch_size, beam_size * num_words)\n",
        "\n",
        "            # Select the top 2 * beam_size scores\n",
        "            next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)\n",
        "\n",
        "            # Prepare the next batch of beam content\n",
        "            next_batch_beam = []\n",
        "\n",
        "            # Iterate over each sentence in the batch\n",
        "            for sent_id in range(batch_size):\n",
        "                # Check if the sentence is already finished\n",
        "                done[sent_id] = done[sent_id] or generated_hyps[sent_id].is_done(next_scores[sent_id].max().item())\n",
        "                if done[sent_id]:\n",
        "                    # If done, pad the batch for this sentence\n",
        "                    next_batch_beam.extend([(0, self.pad_idx, 0)] * beam_size)\n",
        "                    continue\n",
        "\n",
        "                # Prepare the next sentence's beam content\n",
        "                next_sent_beam = []\n",
        "\n",
        "                # Process each potential next word for this sentence\n",
        "                for idx, value in zip(next_words[sent_id], next_scores[sent_id]):\n",
        "                    beam_id = idx // num_words\n",
        "                    word_id = idx % num_words\n",
        "\n",
        "                    # If end of sentence or reached max length, finalize hypothesis\n",
        "                    if word_id == self.eos_idx or cur_len + 1 == max_len:\n",
        "                        generated_hyps[sent_id].add(generated[:cur_len, sent_id * beam_size + beam_id].clone().cpu(), value.item())\n",
        "                    else:\n",
        "                        next_sent_beam.append((value, word_id, sent_id * beam_size + beam_id))\n",
        "\n",
        "                    # Ensure the beam is full\n",
        "                    if len(next_sent_beam) == beam_size:\n",
        "                        break\n",
        "\n",
        "                # Pad the batch if needed\n",
        "                assert len(next_sent_beam) == 0 if cur_len + 1 == max_len else beam_size\n",
        "                if len(next_sent_beam) == 0:\n",
        "                    next_sent_beam = [(0, self.pad_idx, 0)] * beam_size\n",
        "                next_batch_beam.extend(next_sent_beam)\n",
        "\n",
        "            # Sanity check on the next batch\n",
        "            assert len(next_batch_beam) == batch_size * beam_size\n",
        "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
        "            beam_words = generated.new([x[1] for x in next_batch_beam])\n",
        "            beam_idx = src_len.new([x[2] for x in next_batch_beam])\n",
        "\n",
        "            # Re-order batch and update internal states\n",
        "            generated = generated[:, beam_idx]\n",
        "            generated[cur_len] = beam_words\n",
        "            for k in self.cache.keys():\n",
        "                if k != \"slen\":\n",
        "                    self.cache[k] = (self.cache[k][0][beam_idx], self.cache[k][1][beam_idx])\n",
        "\n",
        "            # Update current length\n",
        "            cur_len += 1\n",
        "\n",
        "            # Break if all sentences are done\n",
        "            if all(done):\n",
        "                break\n",
        "\n",
        "        # Select the best hypothesis for each sentence\n",
        "        tgt_len = src_len.new(batch_size)\n",
        "        best = []\n",
        "        for i, hypotheses in enumerate(generated_hyps):\n",
        "            best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
        "            tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
        "            best.append(best_hyp)\n",
        "\n",
        "        # Prepare the final output\n",
        "        decoded = src_len.new(tgt_len.max().item(), batch_size).fill_(self.pad_idx)\n",
        "        for i, hypo in enumerate(best):\n",
        "            decoded[: tgt_len[i] - 1, i] = hypo\n",
        "            decoded[tgt_len[i] - 1, i] = self.eos_idx\n",
        "\n",
        "        # Final check to ensure all sequences end with <EOS>\n",
        "        assert (decoded == self.eos_idx).sum() == 2 * batch_size\n",
        "\n",
        "        return decoded, tgt_len, generated_hyps\n",
        "\n",
        "class BeamHypotheses(object):\n",
        "\n",
        "    def __init__(self, n_hyp, max_len, len_penalty, early_stopping):\n",
        "        \"\"\"\n",
        "        Initializes the n-best list of hypotheses for beam search.\n",
        "        \"\"\"\n",
        "        self.max_len = max_len - 1  # Exclude <BOS> from max length\n",
        "        self.len_penalty = len_penalty\n",
        "        self.early_stopping = early_stopping\n",
        "        self.n_hyp = n_hyp\n",
        "        self.hyp = []  # List to store hypotheses\n",
        "        self.worst_score = 1e9  # Initially set worst score to a very high value\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of hypotheses stored in the list.\n",
        "        \"\"\"\n",
        "        return len(self.hyp)\n",
        "\n",
        "    def add(self, hyp, sum_logprobs):\n",
        "        \"\"\"\n",
        "        Adds a new hypothesis to the list.\n",
        "        \"\"\"\n",
        "        score = sum_logprobs / len(hyp) ** self.len_penalty\n",
        "        if len(self) < self.n_hyp or score > self.worst_score:\n",
        "            self.hyp.append((score, hyp))\n",
        "            if len(self) > self.n_hyp:\n",
        "                # Remove the worst hypothesis if the list exceeds n_hyp\n",
        "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n",
        "                del self.hyp[sorted_scores[0][1]]\n",
        "                self.worst_score = sorted_scores[1][0]\n",
        "            else:\n",
        "                self.worst_score = min(score, self.worst_score)\n",
        "\n",
        "    def is_done(self, best_sum_logprobs):\n",
        "        \"\"\"\n",
        "        Checks if we are done generating hypotheses for this sentence.\n",
        "        \"\"\"\n",
        "        if len(self) < self.n_hyp:\n",
        "            return False\n",
        "        elif self.early_stopping:\n",
        "            return True\n",
        "        else:\n",
        "            return self.worst_score >= best_sum_logprobs / self.max_len**self.len_penalty"
      ],
      "metadata": {
        "id": "R_LJL4Dj5Xib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_degree = 6\n",
        "int_base = 1000\n",
        "max_len = 1024\n",
        "max_output_len = 512\n",
        "\n",
        "operators = ['+', '-', '*', '/', '^', 'sqrt', 'exp', 'ln', 'sin', 'cos', 'tan', 'asin', 'acos', 'atan', 'Abs']\n",
        "variables = [f\"x{i}\" for i in range(2 * max_degree)]\n",
        "constants = [\"pi\", \"E\"]\n",
        "symbols = [\"I\", \"INT+\", \"INT-\", \"FLOAT+\", \"FLOAT-\", \".\", \"10^\"]\n",
        "elements = [str(i) for i in range(max(10, int_base))]\n",
        "SPECIAL_WORDS = [\"<s>\", \"</s>\", \"<pad>\", \"(\", \")\"] + [f\"<SPECIAL_{i}>\" for i in range(10)]\n",
        "func_separator = \"<SPECIAL_3>\"\n",
        "\n",
        "words = SPECIAL_WORDS + constants + variables + operators + symbols + elements\n",
        "id2word = {i: s for i, s in enumerate(words)}\n",
        "word2id = {s: i for i, s in id2word.items()}\n",
        "n_words = len(words)\n",
        "eos_index = 0\n",
        "pad_index = 1"
      ],
      "metadata": {
        "id": "6-TN8yLQaiPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Helper class for creating datasets.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_workers, path, word2id, train, max_len=1024, max_output_len=512, reload_size=-1, size=None):\n",
        "        super().__init__()\n",
        "        self.num_workers = num_workers\n",
        "        self.path = path\n",
        "        self.train = train\n",
        "        self.count = 0\n",
        "        assert size is None\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.max_output_len = max_output_len\n",
        "        self.word2id = word2id\n",
        "\n",
        "        self.func_separator = \"<SPECIAL_3>\"\n",
        "        self.eos_index = 0\n",
        "        self.pad_index = 1\n",
        "\n",
        "        # reloading from file\n",
        "        if path is not None:\n",
        "            assert os.path.isfile(path)\n",
        "            print(f\"Loading data from {path} ...\")\n",
        "            with io.open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "                # either reload the entire file, or the first N lines (for the training set)\n",
        "                if not train:\n",
        "                    lines = [line.rstrip().split(\"|\") for line in f]\n",
        "                else:\n",
        "                    lines = []\n",
        "                    for i, line in enumerate(f):\n",
        "                        if i == reload_size:\n",
        "                            break\n",
        "                        lines.append(line.rstrip().split(\"|\"))\n",
        "            self.data = [xy.split(\"\\t\") for _, xy in lines]\n",
        "            self.data = [xy for xy in self.data if len(xy) == 2]\n",
        "            print(f\"Loaded {len(self.data)} equations from the disk.\")\n",
        "\n",
        "        # dataset size: infinite iterator for train, finite for validation (default of 5000 if no file provided)\n",
        "        if self.train:\n",
        "            self.size = 1 << 60\n",
        "        elif size is None:\n",
        "            self.size = 5000 if path is None else len(self.data)\n",
        "        else:\n",
        "            assert size > 0\n",
        "            self.size = size\n",
        "\n",
        "    def batch_sequences(self, sequences):\n",
        "        \"\"\"\n",
        "        Take as input a list of n sequences (torch.LongTensor vectors) and return\n",
        "        a tensor of size (slen, n) where slen is the length of the longest\n",
        "        sentence, and a vector lengths containing the length of each sentence.\n",
        "        \"\"\"\n",
        "        lengths = torch.LongTensor([len(s) + 2 for s in sequences])\n",
        "        sent = torch.LongTensor(lengths.max().item(), lengths.size(0)).fill_(self.pad_index)\n",
        "        assert lengths.min().item() > 2\n",
        "\n",
        "        sent[0] = self.eos_index\n",
        "        for i, s in enumerate(sequences):\n",
        "            sent[1 : lengths[i] - 1, i].copy_(s)\n",
        "            sent[lengths[i] - 1, i] = self.eos_index\n",
        "\n",
        "        return sent, lengths\n",
        "\n",
        "    def collate_fn(self, elements):\n",
        "        \"\"\"\n",
        "        Collate samples into a batch.\n",
        "        \"\"\"\n",
        "        x, y = zip(*elements)\n",
        "        nb_eqs = [seq.count(self.func_separator) for seq in x]\n",
        "        x = [torch.LongTensor([self.word2id[w] for w in seq]) for seq in x]\n",
        "        y = [torch.LongTensor([self.word2id[w] for w in seq]) for seq in y]\n",
        "        x, x_len = self.env.batch_sequences(x)\n",
        "        y, y_len = self.env.batch_sequences(y)\n",
        "        return (x, x_len), (y, y_len), torch.LongTensor(nb_eqs)\n",
        "\n",
        "    def get_worker_id(self):\n",
        "        \"\"\"\n",
        "        Get worker ID.\n",
        "        \"\"\"\n",
        "        if not self.train:\n",
        "            return 0\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        assert (worker_info is None) == (self.num_workers == 0)\n",
        "        return 0 if worker_info is None else worker_info.id\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return dataset size.\n",
        "        \"\"\"\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Return a training  by reading it from a file.\n",
        "        \"\"\"\n",
        "        return self.read_sample(index)\n",
        "\n",
        "    def read_sample(self, index):\n",
        "        \"\"\"\n",
        "        Read a sample.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            if self.train:\n",
        "                index = self.env.rng.randint(len(self.data))\n",
        "            x, y = self.data[index]\n",
        "            x = x.split()\n",
        "            y = y.split()\n",
        "            if (self.max_len > 0 and len(x) >= self.max_len) or (self.max_output_len > 0 and len(y) >= self.max_output_len):\n",
        "                index += 1\n",
        "                continue\n",
        "            return x, y"
      ],
      "metadata": {
        "id": "SWL5Zn963yhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_iterator(data_path, num_workers, batch_size, word2id):\n",
        "    \"\"\"\n",
        "    Create a training dataset.\n",
        "    \"\"\"\n",
        "    print(f\"Creating train iterator...\")\n",
        "\n",
        "    if num_workers is None:\n",
        "        num_workers = min(4, os.cpu_count() or 1)\n",
        "\n",
        "    dataset = EnvDataset(path=data_path, num_workers=num_workers, word2id=word2id, train=True)\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        timeout=(0 if num_workers == 0 else 86400),\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=False,\n",
        "        collate_fn=dataset.collate_fn,\n",
        "    )"
      ],
      "metadata": {
        "id": "LPtK85U226Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(dataloader):\n",
        "        \"\"\"\n",
        "        Return a training batch for a specific task.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            batch = next(dataloader)\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                \"An unknown exception occurred when fetching batch. \"\n",
        "            )\n",
        "            raise\n",
        "        return batch"
      ],
      "metadata": {
        "id": "cVSk-Oia2EXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_cuda(*args):\n",
        "    \"\"\"\n",
        "    Move tensors to CUDA.\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return args\n",
        "    return [None if x is None else x.cuda() for x in args]\n",
        "\n",
        "def enc_dec_step(encoder, decoder, dataloader, optimizer, clip_grad_norm=5):\n",
        "    \"\"\"\n",
        "    Encoding / decoding step.\n",
        "    \"\"\"\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # batch\n",
        "    (x1, len1), (x2, len2), nb_ops = get_batch(dataloader)\n",
        "\n",
        "    # cuda\n",
        "    x1, len1, x2, len2 = to_cuda(x1, len1, x2, len2)\n",
        "\n",
        "    # target words to predict\n",
        "    alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)\n",
        "    pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word\n",
        "\n",
        "    y = x2[1:].masked_select(pred_mask[:-1])\n",
        "\n",
        "    assert len(y) == (len2 - 1).sum().item()\n",
        "\n",
        "    # forward / loss\n",
        "    encoded = encoder(\"fwd\", x=x1, lengths=len1, causal_att=False)\n",
        "    decoded = decoder(\"fwd\", x=x2, lengths=len2, causal_att=True, src_enc=encoded.transpose(0, 1), src_len=len1)\n",
        "    _, loss = decoder(\"predict\", tensor=decoded, pred_mask=pred_mask, y=y, get_scores=False)\n",
        "\n",
        "    # check NaN\n",
        "    if (loss != loss).data.any():\n",
        "        print(\"NaN detected\")\n",
        "\n",
        "    parameters = {\n",
        "        k: p for v in [encoder, decoder] for k, p in v.named_parameters() if p.requires_grad\n",
        "    }\n",
        "    model_params = list(parameters.values())\n",
        "\n",
        "    # regular optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    if clip_grad_norm > 0:\n",
        "        clip_grad_norm_(model_params, clip_grad_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def save_checkpoint(name, epoch, n_total_iter, best_metrics, encoder, decoder, optimizer, dump_path=\"\"):\n",
        "        \"\"\"\n",
        "        Save the model / checkpoints.\n",
        "        \"\"\"\n",
        "        path = os.path.join(dump_path, \"%s.pth\" % name)\n",
        "        print(\"Saving %s to %s ...\" % (name, path))\n",
        "\n",
        "        data = {\n",
        "            \"epoch\": epoch,\n",
        "            \"n_total_iter\": n_total_iter,\n",
        "            \"best_metrics\": best_metrics,\n",
        "        }\n",
        "\n",
        "        print(f\"Saving encoder parameters ...\")\n",
        "        data[\"encoder\"] = encoder.state_dict()\n",
        "        print(f\"Saving decoder parameters ...\")\n",
        "        data[\"decoder\"] = decoder.state_dict()\n",
        "\n",
        "        print(\"Saving optimizer ...\")\n",
        "        data[\"optimizer\"] = optimizer.state_dict()\n",
        "\n",
        "        torch.save(data, path)\n",
        "\n",
        "def save_best_model(scores, metrics, best_metrics, epoch, n_total_iter, encoder, decoder, optimizer):\n",
        "    \"\"\"\n",
        "    Save best models according to given validation metrics.\n",
        "    \"\"\"\n",
        "    for metric, biggest in metrics:\n",
        "        if metric not in scores:\n",
        "            print('Metric \"%s\" not found in scores!' % metric)\n",
        "            continue\n",
        "        factor = 1 if biggest else -1\n",
        "        if factor * scores[metric] > factor * best_metrics[metric]:\n",
        "            best_metrics[metric] = scores[metric]\n",
        "            print(\"New best score for %s: %.6f\" % (metric, scores[metric]))\n",
        "            save_checkpoint(\"best-%s\" % metric, epoch, n_total_iter, best_metrics, encoder, decoder, optimizer, dump_path=\"\")\n",
        "\n",
        "def save_periodic(save_periodic, epoch, n_total_iter, best_metrics, encoder, decoder, optimizer):\n",
        "    \"\"\"\n",
        "    Save the models periodically.\n",
        "    \"\"\"\n",
        "    if save_periodic > 0 and epoch % params.save_periodic == 0:\n",
        "        save_checkpoint(\"periodic-%i\" % epoch, epoch, n_total_iter, best_metrics, encoder, decoder, optimizer, dump_path=\"\")"
      ],
      "metadata": {
        "id": "azFI-YOT2M47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = get_parser()\n",
        "params = parser.parse_args()\n",
        "helper_env = ODEEnvironment(params)\n",
        "\n",
        "def idx_to_infix(env, idx, input=True):\n",
        "    \"\"\"\n",
        "    Convert an indexed prefix expression to SymPy.\n",
        "    \"\"\"\n",
        "    prefix = [env.id2word[wid] for wid in idx]\n",
        "    infix = env.input_to_infix(prefix) if input else env.output_to_infix(prefix)\n",
        "    return infix\n",
        "\n",
        "def check_hypothesis(eq):\n",
        "    \"\"\"\n",
        "    Check a hypothesis for a given equation and its solution.\n",
        "    \"\"\"\n",
        "    global helper_env\n",
        "    helper_env.rng = np.random.RandomState(0)\n",
        "    src = [helper_env.id2word[wid] for wid in eq[\"src\"]]\n",
        "    tgt = [helper_env.id2word[wid] for wid in eq[\"tgt\"]]\n",
        "    hyp = [helper_env.id2word[wid] for wid in eq[\"hyp\"]]\n",
        "\n",
        "    try:\n",
        "        is_valid = helper_env.check_lyap_validity(src, hyp, tgt)\n",
        "    except MyTimeoutError:\n",
        "        is_valid = -3\n",
        "    except Exception as e:\n",
        "        is_valid = -4\n",
        "\n",
        "    # update hypothesis\n",
        "    eq[\"src\"] = helper_env.input_to_infix(src)\n",
        "    eq[\"tgt\"] = tgt\n",
        "    eq[\"hyp\"] = hyp\n",
        "    eq[\"is_valid\"] = is_valid\n",
        "    return eq\n",
        "\n",
        "def enc_dec_step_beam(data_type, data_path_idx, scores, encoder, decoder, data_path, max_output_len=512,\n",
        "                      eval_verbose=True, dump_path=\"\", beam_size=50, batch_size_eval=16, beam_length_penalty=1,\n",
        "                      beam_early_stopping=True, size=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Encoding / decoding step with beam generation and SymPy check.\n",
        "    \"\"\"\n",
        "    global helper_env\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    max_beam_length = max_output_len + 2\n",
        "    # evaluation details\n",
        "    if eval_verbose:\n",
        "        eval_path = os.path.join(dump_path, f\"eval.beam.{data_type}.{data_path_idx}.{scores['epoch']}\")\n",
        "        f_export = open(eval_path, \"w\")\n",
        "        print(f\"Writing evaluation results in {eval_path} ...\")\n",
        "\n",
        "    def display_logs(logs, offset):\n",
        "        \"\"\"\n",
        "        Display detailed results about success / fails.\n",
        "        \"\"\"\n",
        "        if eval_verbose == 0:\n",
        "            return\n",
        "        for i, res in sorted(logs.items()):\n",
        "            n_valid = sum([int(v) for _, _, v in res[\"hyps\"]])\n",
        "            s = f\"Equation {offset + i} ({n_valid}/{len(res['hyps'])})\\nsrc={res['src']}\\ntgt={res['tgt']}\\n\"\n",
        "            for hyp, score, valid in res[\"hyps\"]:\n",
        "                if score is None:\n",
        "                    s += f\"{int(valid)} {hyp}\\n\"\n",
        "                else:\n",
        "                    s += f\"{int(valid)} {score :.3e} {hyp}\\n\"\n",
        "            f_export.write(s + \"\\n\")\n",
        "            f_export.flush()\n",
        "\n",
        "    # stats\n",
        "    xe_loss = 0\n",
        "    n_valid = torch.zeros(1000, beam_size, dtype=torch.long)\n",
        "    n_total = torch.zeros(1000, dtype=torch.long)\n",
        "    n_perfect_match = 0\n",
        "    n_correct = 0\n",
        "    n_timeout = 0\n",
        "    n_optim = 0\n",
        "    n_input_err = 0\n",
        "    n_other_err = 0\n",
        "\n",
        "    # iterator\n",
        "    iterator = helper_env.create_test_iterator(\n",
        "        data_type,\n",
        "        \"ode_lyapunov\",\n",
        "        data_path=data_path,\n",
        "        data_path_idx=data_path_idx,\n",
        "        batch_size=batch_size_eval,\n",
        "        params=params,\n",
        "        size=size,\n",
        "    )\n",
        "    eval_size = len(iterator.dataset)\n",
        "\n",
        "    for (x1, len1), (x2, len2), nb_ops in iterator:\n",
        "\n",
        "        # cuda\n",
        "        x1, len1, x2, len2 = to_cuda(x1, len1, x2, len2)\n",
        "\n",
        "        # target words to predict\n",
        "        alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)\n",
        "        pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word\n",
        "        y = x2[1:].masked_select(pred_mask[:-1])\n",
        "        assert len(y) == (len2 - 1).sum().item()\n",
        "\n",
        "        x1_, len1_ = x1, len1\n",
        "\n",
        "        bs = len(len1)\n",
        "\n",
        "        # forward\n",
        "        encoded = encoder(\"fwd\", x=x1_, lengths=len1_, causal_att=False)\n",
        "        decoded = decoder(\"fwd\", x=x2, lengths=len2, causal_att=True, src_enc=encoded.transpose(0, 1), src_len=len1_)\n",
        "        word_scores, loss = decoder(\"predict\", tensor=decoded, pred_mask=pred_mask, y=y, get_scores=True)\n",
        "\n",
        "        # correct outputs per sequence / valid top-1 predictions\n",
        "        t = torch.zeros_like(pred_mask, device=y.device)\n",
        "        t[pred_mask] += word_scores.max(1)[1] == y\n",
        "        valid = (t.sum(0) == len2 - 1).cpu().long()\n",
        "        n_perfect_match += valid.sum().item()\n",
        "\n",
        "        # save evaluation details\n",
        "        beam_log = {}\n",
        "        for i in range(len(len1)):\n",
        "            src = idx_to_infix(helper_env, x1[1 : len1[i] - 1, i].tolist(), True)\n",
        "            tgt = idx_to_infix(helper_env, x2[1 : len2[i] - 1, i].tolist(), False)\n",
        "            if valid[i]:\n",
        "                beam_log[i] = {\"src\": src, \"tgt\": tgt, \"hyps\": [(tgt, None, True)]}\n",
        "\n",
        "        # stats\n",
        "        xe_loss += loss.item() * len(y)\n",
        "        n_valid[:, 0].index_add_(-1, nb_ops, valid)\n",
        "        n_total.index_add_(-1, nb_ops, torch.ones_like(nb_ops))\n",
        "\n",
        "        # continue if everything is correct. if eval_verbose, perform\n",
        "        # a full beam search, even on correct greedy generations\n",
        "        if valid.sum() == len(valid) and eval_verbose < 2:\n",
        "            display_logs(beam_log, offset=n_total.sum().item() - bs)\n",
        "            continue\n",
        "\n",
        "        # invalid top-1 predictions - check if there is a solution in the beam\n",
        "        invalid_idx = (1 - valid).nonzero().view(-1)\n",
        "        print(\n",
        "            f\"({n_total.sum().item()}/{eval_size}) Found {bs - len(invalid_idx)}/{bs} \" f\"valid top-1 predictions. Generating solutions ...\"\n",
        "        )\n",
        "\n",
        "        max_beam_length = max_output_len + 2\n",
        "        # generate\n",
        "        _, _, generations = decoder.generate_beam(\n",
        "            encoded.transpose(0, 1),\n",
        "            len1_,\n",
        "            beam_size=beam_size,\n",
        "            length_penalty=beam_length_penalty,\n",
        "            early_stopping=beam_early_stopping,\n",
        "            max_len=max_beam_length,\n",
        "        )\n",
        "        # prepare inputs / hypotheses to check\n",
        "        # if eval_verbose < 2, no beam search on equations solved greedily\n",
        "        inputs = []\n",
        "        for i in range(len(generations)):\n",
        "            if valid[i] and eval_verbose < 2:\n",
        "                continue\n",
        "            for j, (score, hyp) in enumerate(sorted(generations[i].hyp, key=lambda x: x[0], reverse=True)):\n",
        "                inputs.append(\n",
        "                    {\n",
        "                        \"i\": i,\n",
        "                        \"j\": j,\n",
        "                        \"score\": score,\n",
        "                        \"src\": x1[1 : len1[i] - 1, i].tolist(),\n",
        "                        \"tgt\": x2[1 : len2[i] - 1, i].tolist(),\n",
        "                        \"hyp\": hyp[1:].tolist(),\n",
        "                        \"task\": task,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        # check hypotheses with multiprocessing\n",
        "        outputs = []\n",
        "        with ProcessPoolExecutor(max_workers=20) as executor:\n",
        "            for output in executor.map(check_hypothesis, inputs, chunksize=1):\n",
        "                outputs.append(output)\n",
        "\n",
        "        # read results\n",
        "        for i in range(bs):\n",
        "\n",
        "            # select hypotheses associated to current equation\n",
        "            gens = sorted([o for o in outputs if o[\"i\"] == i], key=lambda x: x[\"j\"])\n",
        "            assert (len(gens) == 0) == (valid[i] and eval_verbose < 2) and (i in beam_log) == valid[i]\n",
        "            if len(gens) == 0:\n",
        "                continue\n",
        "\n",
        "            # source / target\n",
        "            src = gens[0][\"src\"]\n",
        "            tgt = gens[0][\"tgt\"]\n",
        "            beam_log[i] = {\"src\": src, \"tgt\": tgt, \"hyps\": []}\n",
        "\n",
        "            # for each hypothesis\n",
        "            for j, gen in enumerate(gens):\n",
        "\n",
        "                # sanity check\n",
        "                assert gen[\"src\"] == src and gen[\"tgt\"] == tgt and gen[\"i\"] == i and gen[\"j\"] == j\n",
        "\n",
        "                # if the hypothesis is correct, and we did not find a correct one before\n",
        "                is_valid = gen[\"is_valid\"]\n",
        "                if is_valid == 1 and not valid[i]:\n",
        "                    n_valid[nb_ops[i], j] += 1\n",
        "                    valid[i] = 1\n",
        "\n",
        "                # update beam log\n",
        "                beam_log[i][\"hyps\"].append((gen[\"hyp\"], gen[\"score\"], is_valid))\n",
        "                if j == 0:\n",
        "                    n_correct += is_valid != -2\n",
        "                    n_timeout += is_valid == -3\n",
        "                    n_optim += is_valid == -1\n",
        "                    n_input_err += is_valid == -5\n",
        "                    n_other_err += is_valid == -4\n",
        "\n",
        "        # valid solutions found with beam search\n",
        "        print(f\"    Found {valid.sum().item()}/{bs} solutions in beam hypotheses.\")\n",
        "\n",
        "        # export evaluation details\n",
        "        if eval_verbose:\n",
        "            assert len(beam_log) == bs\n",
        "            display_logs(beam_log, offset=n_total.sum().item() - bs)\n",
        "\n",
        "    # evaluation details\n",
        "    if eval_verbose:\n",
        "        f_export.close()\n",
        "        print(f\"Evaluation results written in {eval_path}\")\n",
        "\n",
        "    # log\n",
        "    _n_valid = n_valid.sum().item()\n",
        "    _n_total = n_total.sum().item()\n",
        "    print(f\"{_n_valid}/{_n_total} ({100. * _n_valid / _n_total}%) equations were evaluated correctly.\")\n",
        "\n",
        "    # compute perplexity and prediction accuracy\n",
        "    assert _n_total == eval_size\n",
        "\n",
        "    data_path_idx_scores = \"\"\n",
        "    task = \"ode_lyapunov\"\n",
        "    scores[f\"{data_type}_{task}_{data_path_idx_scores}xe_loss\"] = xe_loss / _n_total\n",
        "    scores[f\"{data_type}_{task}_{data_path_idx_scores}beam_acc\"] = 100.0 * _n_valid / _n_total\n",
        "    scores[f\"{data_type}_{task}_{data_path_idx_scores}perfect\"] = 100.0 * n_perfect_match / _n_total\n",
        "    scores[f\"{data_type}_{task}_{data_path_idx_scores}correct\"] = 100.0 * (n_perfect_match + n_correct) / _n_total\n",
        "\n",
        "    scores[f\"{data_type}_{task}_{data_path_idx_scores}optim\"] = 100.0 * n_optim / _n_total\n",
        "    scores[f\"{data_type}_{task}_{data_path_idx_scores}timeout\"] = 100.0 * n_timeout / _n_total\n",
        "    scores[f\"{data_type}_{task}_{data_path_idx_scores}input_err\"] = 100.0 * n_input_err / _n_total\n",
        "    scores[f\"{data_type}_{task}_{data_path_idx_scores}other_err\"] = 100.0 * n_other_err / _n_total\n",
        "\n",
        "    # per class perplexity and prediction accuracy\n",
        "    for i in range(len(n_total)):\n",
        "        if n_total[i].item() == 0:\n",
        "            continue\n",
        "        print(f\"{i}: {n_valid[i].sum().item()} / {n_total[i].item()} \" f\"({100. * n_valid[i].sum().item() / max(n_total[i].item(), 1)}%)\")\n",
        "        scores[f\"{data_type}_{task}_{data_path_idx_scores}beam_acc_{i}\"] = 100.0 * n_valid[i].sum().item() / max(n_total[i].item(), 1)"
      ],
      "metadata": {
        "id": "DwA1KKCVB-5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_all_evals(epoch, encoder, decoder, eval_size):\n",
        "        \"\"\"\n",
        "        Run all evaluations.\n",
        "        \"\"\"\n",
        "        scores = OrderedDict({\"epoch\": epoch})\n",
        "        with torch.no_grad():\n",
        "            eval_tasks = [[\"valid\", 1]]\n",
        "            for data_type, data_path_idx in eval_tasks:\n",
        "                enc_dec_step_beam(data_type, data_path_idx, scores, encoder, decoder, eval_size)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "mVlePhe96Xf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "max_epoch = 3\n",
        "epoch = 0\n",
        "epoch_size = 200\n",
        "n_total_iter = 0\n",
        "eval_size =\n",
        "\n",
        "# validation metrics\n",
        "validation_metrics = \"valid_ode_lyapunov_beam_acc\"\n",
        "metrics = []\n",
        "metrics = [m for m in validation_metrics.split(\",\") if m != \"\"]\n",
        "for m in metrics:\n",
        "    m = (m[1:], False) if m[0] == \"_\" else (m, True)\n",
        "    metrics.append(m)\n",
        "best_metrics = {metric: (-1e12 if biggest else 1e12) for (metric, biggest) in metrics}\n",
        "\n",
        "batch_size = 16\n",
        "data_path = \"FLyap.txt\"\n",
        "dataloader = iter(create_train_iterator(data_path=data_path, num_workers=None, batch_size=batch_size))\n",
        "optimizer =\n",
        "\n",
        "encoder = TransformerModel(params, id2word, is_encoder=True, with_output=False)\n",
        "decoder = TransformerModel(params, id2word, is_encoder=False, with_output=True)\n",
        "\n",
        "last_time = time.time()\n",
        "\n",
        "for _ in range(max_epoch):\n",
        "\n",
        "    print(\"============ Starting epoch %i ... ============\" % epoch)\n",
        "\n",
        "    n_equations = 0\n",
        "    n_iter = 0\n",
        "\n",
        "    while n_equations < epoch_size:\n",
        "\n",
        "        # training steps\n",
        "        loss = enc_dec_step(encoder, decoder, dataloader, optimizer)\n",
        "        n_equations += batch_size\n",
        "        n_iter += 1\n",
        "        n_total_iter += 1 #print some stats\n",
        "\n",
        "        if n_total_iter % 20 == 0:\n",
        "          new_time = time.time()\n",
        "          diff = new_time - last_time\n",
        "          print('Training speed is ', 20/diff, ' iter/s. The loss in ', n_iter, '. iteration: ', loss.item())\n",
        "          last_time = new_time\n",
        "\n",
        "    print(\"============ End of epoch %i ============\" % epoch)\n",
        "\n",
        "    # evaluate perplexity\n",
        "    scores = run_all_evals(epoch, encoder, decoder, eval_size)\n",
        "\n",
        "    # end of epoch\n",
        "    save_best_model(scores, metrics, best_metrics, epoch, n_total_iter, encoder, decoder, optimizer)\n",
        "    save_periodic(save_periodic, epoch, n_total_iter, best_metrics, encoder, decoder, optimizer)\n",
        "    save_checkpoint(\"checkpoint\", epoch, n_total_iter, best_metrics, encoder, decoder, optimizer, dump_path=\"\")\n",
        "    epoch += 1"
      ],
      "metadata": {
        "id": "NNH6kg6hNjlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate perplexity\n",
        "if params.is_master:\n",
        "    scores = evaluator.run_all_evals()\n",
        "    logger.info(scores)\n",
        "\n",
        "    # print / JSON log\n",
        "    for k, v in scores.items():\n",
        "        logger.info(\"%s -> %.6f\" % (k, v))\n",
        "    logger.info(\"__log__:%s\" % json.dumps(scores))\n",
        "\n",
        "    # end of epoch\n",
        "    trainer.save_best_model(scores)\n",
        "    trainer.save_periodic()\n",
        "    trainer.end_epoch(scores)"
      ],
      "metadata": {
        "id": "u7aJaBmLvYwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sympy as sp\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.children = []\n",
        "\n",
        "    def add_child(self, child):\n",
        "        self.children.append(child)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"TreeNode({self.value}, {self.children})\"\n",
        "\n",
        "class MathTokenizer:\n",
        "    def __init__(self, params, max_degree=5, int_base=1000):\n",
        "        self.max_degree = max_degree\n",
        "        self.int_base = int_base\n",
        "\n",
        "        self.operators = ['+', '-', '*', '/', '^', 'sqrt', 'exp', 'ln', 'sin', 'cos', 'tan', 'asin', 'acos', 'atan', 'Abs']\n",
        "        self.variables = [f\"x{i}\" for i in range(2 * self.max_degree)]\n",
        "        self.constants = [\"pi\", \"E\"]\n",
        "        self.symbols = [\"I\", \"INT+\", \"INT-\", \"FLOAT+\", \"FLOAT-\", \".\", \"10^\"]\n",
        "        self.elements = [str(i) for i in range(max(10, self.int_base))]\n",
        "        self.SPECIAL_WORDS = [\"<s>\", \"</s>\", \"<pad>\", \"(\", \")\"] + [f\"<SPECIAL_{i}>\" for i in range(10)]\n",
        "        self.func_separator = [\"SEP\"]\n",
        "\n",
        "        self.words = self.SPECIAL_WORDS + self.constants + self.variables + self.operators + self.symbols + self.elements + self.func_separator\n",
        "        self.id2word = {i: s for i, s in enumerate(self.words)}\n",
        "        self.word2id = {s: i for i, s in self.id2word.items()}\n",
        "        self.n_words = len(self.words)\n",
        "        self.eos_index = 0\n",
        "        self.pad_index = 1\n",
        "\n",
        "\n",
        "    def parse_expression(expr):\n",
        "        \"\"\"\n",
        "        Parse an infix expression into a tree structure using recursive descent.\n",
        "        Supports numbers, variables, operators (+, -, *, /) and functions (cos, sin).\n",
        "        \"\"\"\n",
        "        tokens = re.findall(r'sqrt|exp|ln|sin|cos|tan|asin|acos|atan|Abs|[+\\-*/^()]|x\\d+|pi|E|\\d+\\.?\\d*', expr)\n",
        "        pos = 0\n",
        "\n",
        "        def peek():\n",
        "            return tokens[pos] if pos < len(tokens) else None\n",
        "\n",
        "        def consume(expected=None):\n",
        "            nonlocal pos\n",
        "            token = tokens[pos]\n",
        "            if expected and token != expected:\n",
        "                raise ValueError(f\"Expected {expected} but got {token}\")\n",
        "            pos += 1\n",
        "            return token\n",
        "\n",
        "        def parse_primary():\n",
        "            token = peek()\n",
        "            if token is None:\n",
        "                raise ValueError(\"Unexpected end of expression\")\n",
        "            if token == '(':\n",
        "                consume('(')\n",
        "                node = parse_expr()\n",
        "                consume(')')\n",
        "                return node\n",
        "            elif token in {'cos', 'sin'}:\n",
        "                # Function call: function may be followed by a parenthesized argument.\n",
        "                func = consume()\n",
        "                node = TreeNode(func)\n",
        "                if peek() == '(':\n",
        "                    consume('(')\n",
        "                    node.add_child(parse_expr())\n",
        "                    consume(')')\n",
        "                else:\n",
        "                    node.add_child(parse_primary())\n",
        "                return node\n",
        "            else:\n",
        "                # variable or constant\n",
        "                return TreeNode(consume())\n",
        "\n",
        "        def parse_factor():\n",
        "            # For now, factor is simply a primary; we assume no unary '-' separate from number tokens.\n",
        "            return parse_primary()\n",
        "\n",
        "        def parse_term():\n",
        "            node = parse_factor()\n",
        "            while peek() in ('*', '/'):\n",
        "                op = consume()\n",
        "                new_node = TreeNode(op)\n",
        "                new_node.add_child(node)\n",
        "                new_node.add_child(parse_factor())\n",
        "                node = new_node\n",
        "            return node\n",
        "\n",
        "        def parse_expr():\n",
        "            node = parse_term()\n",
        "            while peek() in ('+', '-'):\n",
        "                op = consume()\n",
        "                new_node = TreeNode(op)\n",
        "                new_node.add_child(node)\n",
        "                new_node.add_child(parse_term())\n",
        "                node = new_node\n",
        "            return node\n",
        "\n",
        "        return parse_expr()\n",
        "\n",
        "    def enumerate_tree(self, tree):\n",
        "        \"\"\"\n",
        "        Enumerate the tree in Polish (pre-order) notation.\n",
        "        \"\"\"\n",
        "        if not tree:\n",
        "            return []\n",
        "        result = [tree.value]\n",
        "        for child in tree.children:\n",
        "            result.extend(self.enumerate_tree(child))\n",
        "        return result\n",
        "\n",
        "    def tokenize_integer(self, n):\n",
        "        \"\"\"\n",
        "        Tokenize an integer in base 1000.\n",
        "        \"\"\"\n",
        "        if n == 0:\n",
        "            return ['0']\n",
        "        tokens = []\n",
        "        sign = '+' if n >= 0 else '-'\n",
        "        n = abs(n)\n",
        "        while n > 0:\n",
        "            remainder = n % 1000\n",
        "            tokens.append(str(remainder))\n",
        "            n = n // 1000\n",
        "        tokens.append(sign)\n",
        "        return tokens[::-1]  # Reverse to get the correct order\n",
        "\n",
        "    def tokenize_real(self, x):\n",
        "        \"\"\"\n",
        "        Tokenize a real number in scientific notation.\n",
        "        For example, 2.1 is represented as 21 * 10^(-1)\n",
        "        \"\"\"\n",
        "        sign = '+' if x >= 0 else '-'\n",
        "        x = abs(x)\n",
        "        s = str(x)\n",
        "        if '.' in s:\n",
        "            integer_part, fractional_part = s.split('.')\n",
        "            # Remove any trailing zeros for correct exponent computation:\n",
        "            fractional_part = fractional_part.rstrip('0')\n",
        "            if fractional_part == '':\n",
        "                mantissa = integer_part\n",
        "                exponent = 0\n",
        "            else:\n",
        "                mantissa = integer_part + fractional_part\n",
        "                exponent = -len(fractional_part)\n",
        "        else:\n",
        "            mantissa = s\n",
        "            exponent = 0\n",
        "        return self.tokenize_integer(int(mantissa)) + ['10^'] + self.tokenize_integer(exponent)\n",
        "\n",
        "    def tokenize_expression(self, expression):\n",
        "        \"\"\"\n",
        "        Convert the enumerated expression (in Polish notation) into tokens.\n",
        "        Handles operators, variables, and constants (both integer and real).\n",
        "        \"\"\"\n",
        "        tokens = []\n",
        "        for token in expression:\n",
        "            if token in OPERATORS or token in VARIABLES:\n",
        "                tokens.append(token)\n",
        "            elif re.match(r'^-?\\d+$', token):  # Integer constant\n",
        "                tokens.extend(self.tokenize_integer(int(token)))\n",
        "            elif re.match(r'^-?\\d+\\.\\d*$', token):  # Real constant\n",
        "                tokens.extend(self.tokenize_real(float(token)))\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown token: {token}\")\n",
        "        return tokens\n",
        "\n",
        "    def tokenize(self, expressions):\n",
        "        \"\"\"\n",
        "        Tokenize a list of expressions.\n",
        "        \"\"\"\n",
        "        tokenized_expressions = []\n",
        "        for expression in expressions:\n",
        "            tree = self.parse_expression(expression)\n",
        "            enumerated = self.enumerate_tree(tree)\n",
        "            tokens = self.tokenize_expression(enumerated)\n",
        "            tokenized_expressions.extend(tokens + self.func_separator)\n",
        "        return tokenized_expressions[:-1]  # Remove the last SEP\n",
        "\n",
        "# Example usage\n",
        "expressions = [\n",
        "    \"cos(2.1 * x0) * (x1 + 2)\",\n",
        "    \"sin(3 * x1 + 2)\"\n",
        "]\n",
        "\n",
        "tokenized = tokenize(expressions)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "c7H8NLVW4sGl",
        "outputId": "aa1e7b0d-865a-4dd6-f96e-ed601199de06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenize' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3cb84df30b06>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m ]\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpressions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1, layer_id=None):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.layer_id = layer_id  # Unique identifier for caching\n",
        "        self.dropout = dropout\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # KV cache dictionary\n",
        "        self.cache = {}\n",
        "\n",
        "    def reset_cache(self):\n",
        "        \"\"\"Reset the KV cache.\"\"\"\n",
        "        self.cache = {}\n",
        "\n",
        "    def forward(self, query, key=None, value=None, mask=None, is_causal=False, use_cache=False):\n",
        "        batch_size, qlen, dim = query.shape\n",
        "\n",
        "        # If key and value are not provided, assume self-attention\n",
        "        if key is None:\n",
        "            key = query\n",
        "        if value is None:\n",
        "            value = query\n",
        "\n",
        "        # Linear transformations\n",
        "        Q = self.query(query)  # (batch_size, qlen, embed_dim)\n",
        "        K = self.key(key)      # (batch_size, klen, embed_dim)\n",
        "        V = self.value(value)  # (batch_size, klen, embed_dim)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, qlen, head_dim)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, klen, head_dim)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, klen, head_dim)\n",
        "\n",
        "        # KV caching logic: for self-attention in decoding, accumulate keys/values if qlen==1\n",
        "        if use_cache:\n",
        "            if self.layer_id in self.cache:\n",
        "                cached_K, cached_V = self.cache[self.layer_id]\n",
        "                if qlen == 1:  # incremental decoding: append new keys/values\n",
        "                    K = torch.cat([cached_K, K], dim=2)\n",
        "                    V = torch.cat([cached_V, V], dim=2)\n",
        "                else:\n",
        "                    # For full sequence processing, reuse cached values\n",
        "                    K, V = cached_K, cached_V\n",
        "            self.cache[self.layer_id] = (K, V)\n",
        "\n",
        "        klen = K.size(2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        Q = Q / math.sqrt(self.head_dim)\n",
        "        scores = torch.matmul(Q, K.transpose(2, 3))  # (batch_size, num_heads, qlen, klen)\n",
        "\n",
        "        # Apply causal mask if needed\n",
        "        if is_causal:\n",
        "            causal_mask = torch.triu(torch.ones(qlen, klen, device=query.device), diagonal=1).bool()\n",
        "            scores = scores.masked_fill(causal_mask, -float(\"inf\"))\n",
        "\n",
        "        # Apply additional mask if provided\n",
        "        if mask is not None:\n",
        "            # If mask is 3D assume (batch, qlen, klen), else 2D assumed to be (batch, klen)\n",
        "            if mask.dim() == 3:\n",
        "                mask = mask.unsqueeze(1)  # (batch, 1, qlen, klen)\n",
        "            else:\n",
        "                mask = mask.unsqueeze(1).unsqueeze(1)  # (batch, 1, 1, klen)\n",
        "            # Assume mask==0 means masked position.\n",
        "            scores = scores.masked_fill((mask == 0), -float(\"inf\"))\n",
        "\n",
        "        # Compute attention weights\n",
        "        weights = F.softmax(scores.float(), dim=-1).type_as(scores)\n",
        "        weights = F.dropout(weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Compute context vector and reassemble\n",
        "        context = torch.matmul(weights, V)  # (batch_size, num_heads, qlen, head_dim)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
        "        output = self.out(context)\n",
        "        return output"
      ],
      "metadata": {
        "id": "yVRtbjZgYcwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim (int): Dimensionality of the input and output.\n",
        "            ff_dim (int): Dimensionality of the hidden layer.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, ff_dim)  # First linear transformation\n",
        "        self.linear2 = nn.Linear(ff_dim, embed_dim)  # Second linear transformation\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()  # Non-linear activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, seq_len, embed_dim).\n",
        "        \"\"\"\n",
        "        # First linear layer + activation\n",
        "        x = self.activation(self.linear1(x))  # (batch_size, seq_len, ff_dim)\n",
        "        # Second linear layer + dropout\n",
        "        x = self.linear2(x)  # (batch_size, seq_len, embed_dim)\n",
        "        x = self.dropout(x)  # Apply dropout after the second linear layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "n1d5v4-xtrzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1, is_decoder=False, layer_id=None):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        # For decoder blocks, use distinct layer IDs for self and cross attention\n",
        "        if is_decoder:\n",
        "            self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout, layer_id=f\"{layer_id}_self\")\n",
        "            self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dropout, layer_id=f\"{layer_id}_cross\")\n",
        "            self.norm2 = nn.LayerNorm(embed_dim)\n",
        "            self.dropout2 = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout, layer_id=layer_id)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.feed_forward = FeedForward(embed_dim, ff_dim, dropout)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output=None, src_mask=None, tgt_mask=None, use_cache=False):\n",
        "        # Self-attention\n",
        "        attn_output = self.self_attn(x, mask=tgt_mask, is_causal=self.is_decoder, use_cache=use_cache)\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Cross-attention for decoder blocks\n",
        "        if self.is_decoder:\n",
        "            if encoder_output is None:\n",
        "                raise ValueError(\"encoder_output must be provided for decoder blocks\")\n",
        "            # Use src_mask generated from source lengths (if available) for cross-attention\n",
        "            cross_attn_output = self.cross_attn(x, key=encoder_output, value=encoder_output, mask=src_mask, use_cache=use_cache)\n",
        "            x = x + self.dropout2(cross_attn_output)\n",
        "            x = self.norm2(x)\n",
        "\n",
        "        # Feedforward network\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout3(ff_output)\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ljsQWFvkemor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1, is_decoder=False, padding_idx=None):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.is_decoder = is_decoder\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        # Embedding layers\n",
        "        self.src_embed = nn.Embedding(src_vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "        self.tgt_embed = nn.Embedding(tgt_vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 1000, embed_dim))  # Learned positional embeddings\n",
        "\n",
        "        self.embed_norm = nn.LayerNorm(embed_dim)\n",
        "        self.embed_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoder stack\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout, is_decoder=False, layer_id=i)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        # Decoder stack (if applicable)\n",
        "        if self.is_decoder:\n",
        "            self.decoder = nn.ModuleList([\n",
        "                TransformerBlock(embed_dim, num_heads, ff_dim, dropout, is_decoder=True, layer_id=i)\n",
        "                for i in range(num_layers)\n",
        "            ])\n",
        "\n",
        "        # Final output layer\n",
        "        self.fc_out = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "\n",
        "    def reset_cache(self):\n",
        "        \"\"\"Reset the KV cache for all MultiHeadAttention modules.\"\"\"\n",
        "        for block in self.encoder:\n",
        "            block.self_attn.reset_cache()\n",
        "        if self.is_decoder:\n",
        "            for block in self.decoder:\n",
        "                block.self_attn.reset_cache()\n",
        "                block.cross_attn.reset_cache()\n",
        "\n",
        "    def generate_padding_mask(self, x):\n",
        "        \"\"\"\n",
        "        Generate a padding mask for the input tensor `x` of shape (bs, slen).\n",
        "        Returns a mask of shape (bs, slen) where non-padding tokens are 1.\n",
        "        \"\"\"\n",
        "        return (x != self.padding_idx).float()\n",
        "\n",
        "    def generate_src_mask(self, src_len, max_src_len=None):\n",
        "        \"\"\"\n",
        "        Generate a source mask for the encoder output.\n",
        "        Args:\n",
        "            src_len (torch.Tensor): Lengths of the source sequences.\n",
        "            max_src_len (int, optional): Maximum source length to consider.\n",
        "        Returns:\n",
        "            torch.Tensor: A boolean mask of shape (bs, max_src_len) where True indicates valid tokens.\n",
        "        \"\"\"\n",
        "        if max_src_len is not None:\n",
        "            src_len = torch.clamp(src_len, max=max_src_len)\n",
        "        max_len = int(src_len.max().item())\n",
        "        return torch.arange(max_len, device=src_len.device)[None, :] < src_len[:, None]\n",
        "\n",
        "    def forward(self, src, src_len=None, tgt=None, src_mask=None, tgt_mask=None, use_cache=False, compute_loss=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src (torch.Tensor): Source sequence (bs, src_len).\n",
        "            src_len (torch.Tensor, optional): Lengths of the source sequences.\n",
        "            tgt (torch.Tensor, optional): Target sequence (bs, tgt_len).\n",
        "            src_mask (torch.Tensor, optional): External mask for the encoder input.\n",
        "            tgt_mask (torch.Tensor, optional): External mask for the decoder input.\n",
        "            use_cache (bool): Whether to use KV caching.\n",
        "            compute_loss (bool): Whether to compute the cross-entropy loss.\n",
        "        Returns:\n",
        "            encoder_output, decoder_output, loss (if computed)\n",
        "        \"\"\"\n",
        "        # Reset cache at the start of a new sequence if caching is enabled\n",
        "        if use_cache:\n",
        "            self.reset_cache()\n",
        "\n",
        "        # Generate padding masks from input tokens\n",
        "        src_pad_mask = self.generate_padding_mask(src)\n",
        "        if tgt is not None:\n",
        "            tgt_pad_mask = self.generate_padding_mask(tgt)\n",
        "        else:\n",
        "            tgt_pad_mask = None\n",
        "\n",
        "        # Generate source mask from lengths for cross-attention (if provided)\n",
        "        if self.is_decoder and src_len is not None:\n",
        "            src_enc_mask = self.generate_src_mask(src_len)\n",
        "        else:\n",
        "            src_enc_mask = src_pad_mask  # fallback to padding mask\n",
        "\n",
        "        # Source embeddings with positional encoding\n",
        "        src_seq_len = src.size(1)\n",
        "        src_emb = self.src_embed(src) + self.pos_embed[:, :src_seq_len, :]\n",
        "        src_emb = self.embed_norm(src_emb)\n",
        "        src_emb = self.embed_dropout(src_emb)\n",
        "\n",
        "        # Encoder pass\n",
        "        encoder_output = src_emb\n",
        "        for layer in self.encoder:\n",
        "            encoder_output = layer(encoder_output, src_mask=src_pad_mask, use_cache=use_cache)\n",
        "\n",
        "        # If not a decoder, return encoder output only\n",
        "        if not self.is_decoder:\n",
        "            return encoder_output, None\n",
        "\n",
        "        # For decoder: preserve original target indices for loss computation\n",
        "        original_tgt = tgt.clone()  # Save token indices before embedding\n",
        "\n",
        "        # Target embeddings with positional encoding\n",
        "        tgt_seq_len = tgt.size(1)\n",
        "        tgt_emb = self.tgt_embed(tgt) + self.pos_embed[:, :tgt_seq_len, :]\n",
        "        tgt_emb = self.embed_norm(tgt_emb)\n",
        "        tgt_emb = self.embed_dropout(tgt_emb)\n",
        "\n",
        "        # Decoder pass\n",
        "        decoder_output = tgt_emb\n",
        "        for layer in self.decoder:\n",
        "            # Pass src_enc_mask to cross-attention instead of src_pad_mask if available\n",
        "            decoder_output = layer(decoder_output, encoder_output=encoder_output,\n",
        "                                   src_mask=src_enc_mask, tgt_mask=tgt_pad_mask, use_cache=use_cache)\n",
        "\n",
        "        # Final projection to vocabulary\n",
        "        output = self.fc_out(decoder_output)  # (bs, tgt_len, tgt_vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if compute_loss:\n",
        "            # Flatten predictions and original target token indices\n",
        "            output_flat = output.view(-1, self.fc_out.out_features)\n",
        "            tgt_flat = original_tgt.view(-1)\n",
        "            # Exclude padding positions from loss computation\n",
        "            non_pad_mask = tgt_flat != self.padding_idx\n",
        "            output_flat = output_flat[non_pad_mask]\n",
        "            tgt_flat = tgt_flat[non_pad_mask]\n",
        "            loss = F.cross_entropy(output_flat, tgt_flat, reduction=\"mean\")\n",
        "\n",
        "        return encoder_output, output, loss"
      ],
      "metadata": {
        "id": "cKfdqKQUekXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}